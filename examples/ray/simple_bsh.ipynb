{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# TensorTrade - Renderers and Plotly Visualization Chart\n",
    "## Data Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ipywidgets is required to run Plotly in Jupyter Notebook.\n",
    "# Uncomment and run the following line to install it if required.\n",
    "\n",
    "#!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensortrade.oms.instruments import Instrument\n",
    "\n",
    "USD = Instrument(\"USD\", 2, \"U.S. Dollar\")\n",
    "TTC = Instrument(\"TTC\", 8, \"TensorTrade Coin\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete\n",
    "\n",
    "from tensortrade.env.default.actions import TensorTradeActionScheme\n",
    "\n",
    "from tensortrade.env.generic import ActionScheme, TradingEnv\n",
    "from tensortrade.core import Clock\n",
    "from tensortrade.oms.instruments import ExchangePair\n",
    "from tensortrade.oms.wallets import Portfolio\n",
    "from tensortrade.oms.orders import (\n",
    "    Order,\n",
    "    proportion_order,\n",
    "    TradeSide,\n",
    "    TradeType\n",
    ")\n",
    "\n",
    "\n",
    "class BSH(TensorTradeActionScheme):\n",
    "\n",
    "    registered_name = \"bsh\"\n",
    "\n",
    "    def __init__(self, cash: 'Wallet', asset: 'Wallet'):\n",
    "        super().__init__()\n",
    "        self.cash = cash\n",
    "        self.asset = asset\n",
    "\n",
    "        self.listeners = []\n",
    "        self.action = 0\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return Discrete(2)\n",
    "\n",
    "    def attach(self, listener):\n",
    "        self.listeners += [listener]\n",
    "        return self\n",
    "\n",
    "    def get_orders(self, action: int, portfolio: 'Portfolio'):\n",
    "        order = None\n",
    "\n",
    "        if abs(action - self.action) > 0:\n",
    "            src = self.cash if self.action == 0 else self.asset\n",
    "            tgt = self.asset if self.action == 0 else self.cash\n",
    "            order = proportion_order(portfolio, src, tgt, 1.0)\n",
    "            self.action = action\n",
    "\n",
    "        for listener in self.listeners:\n",
    "            listener.on_action(action)\n",
    "\n",
    "        return [order]\n",
    "\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.action = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensortrade.env.default.rewards import TensorTradeRewardScheme\n",
    "from tensortrade.feed.core import Stream, DataFeed\n",
    "\n",
    "\n",
    "class PBR(TensorTradeRewardScheme):\n",
    "\n",
    "    registered_name = \"pbr\"\n",
    "\n",
    "    def __init__(self, price: 'Stream'):\n",
    "        super().__init__()\n",
    "        self.position = -1\n",
    "\n",
    "        r = Stream.sensor(price, lambda p: p.value, dtype=\"float\").diff()\n",
    "        position = Stream.sensor(self, lambda rs: rs.position, dtype=\"float\")\n",
    "\n",
    "        reward = (r * position).fillna(0).rename(\"reward\")\n",
    "\n",
    "        self.feed = DataFeed([reward])\n",
    "        self.feed.compile()\n",
    "\n",
    "    def on_action(self, action: int):\n",
    "        self.position = -1 if action == 0 else 1\n",
    "\n",
    "    def get_reward(self, portfolio: 'Portfolio'):\n",
    "        return self.feed.next()[\"reward\"]\n",
    "\n",
    "    def reset(self):\n",
    "        self.position = -1\n",
    "        self.feed.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensortrade.env.generic import Renderer\n",
    "\n",
    "\n",
    "class PositionChangeChart(Renderer):\n",
    "\n",
    "    def __init__(self, color: str = \"orange\"):\n",
    "        self.color = \"orange\"\n",
    "\n",
    "    def render(self, env, **kwargs):\n",
    "        history = pd.DataFrame(env.observer.renderer_history)\n",
    "\n",
    "        actions = list(history.action)\n",
    "        p = list(history.price)\n",
    "\n",
    "        buy = {}\n",
    "        sell = {}\n",
    "\n",
    "        for i in range(len(actions) - 1):\n",
    "            a1 = actions[i]\n",
    "            a2 = actions[i + 1]\n",
    "\n",
    "            if a1 != a2:\n",
    "                if a1 == 0 and a2 == 1:\n",
    "                    buy[i] = p[i]\n",
    "                else:\n",
    "                    sell[i] = p[i]\n",
    "\n",
    "        buy = pd.Series(buy)\n",
    "        sell = pd.Series(sell)\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        fig.suptitle(\"Performance\")\n",
    "\n",
    "        axs[0].plot(np.arange(len(p)), p, label=\"price\", color=self.color)\n",
    "        axs[0].scatter(buy.index, buy.values, marker=\"^\", color=\"green\")\n",
    "        axs[0].scatter(sell.index, sell.values, marker=\"^\", color=\"red\")\n",
    "        axs[0].set_title(\"Trading Chart\")\n",
    "\n",
    "        performance = pd.DataFrame.from_dict(env.action_scheme.portfolio.performance, orient='index')\n",
    "        performance.plot(ax=axs[1])\n",
    "        axs[1].set_title(\"Net Worth\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-18 15:28:00,666\tINFO services.py:1172 -- View the Ray dashboard at \u001B[1m\u001B[32mhttp://127.0.0.1:8265\u001B[39m\u001B[22m\n",
      "2021-04-18 15:28:04,294\tWARNING worker.py:1107 -- The dashboard on node LAPTOP-20JJPQJR failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ray\\new_dashboard\\dashboard.py\", line 187, in <module>\n",
      "    dashboard = Dashboard(\n",
      "  File \"c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ray\\new_dashboard\\dashboard.py\", line 81, in __init__\n",
      "    build_dir = setup_static_dir()\n",
      "  File \"c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ray\\new_dashboard\\dashboard.py\", line 38, in setup_static_dir\n",
      "    raise OSError(\n",
      "FileNotFoundError: [Errno 2] Dashboard build directory not found. If installing from source, please follow the additional steps required to build the dashboard(cd python/ray/new_dashboard/client && npm install && npm ci && npm run build): 'c:\\\\users\\\\xichao.chen\\\\miniconda3\\\\envs\\\\python38\\\\lib\\\\site-packages\\\\ray\\\\new_dashboard\\\\client\\\\build'\n",
      "\n",
      "2021-04-18 15:28:06,245\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m WARNING:tensorflow:From c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m Instructions for updating:\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m non-resource variables are not supported in the long term\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m WARNING:tensorflow:From c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m Instructions for updating:\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m non-resource variables are not supported in the long term\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:28:19,568\tDEBUG rollout_worker.py:1075 -- Creating policy for default_policy\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:28:19,569\tDEBUG catalog.py:618 -- Created preprocessor <ray.rllib.models.preprocessors.NoPreprocessor object at 0x000001DC8404A220>: Box(-inf, inf, (25, 5), float32) -> (25, 5)\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:28:19,574\tWARNING deprecation.py:33 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:28:19,689\tINFO torch_policy.py:109 -- TorchPolicy running on GPU.\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:24,295\tDEBUG rollout_worker.py:1075 -- Creating policy for default_policy\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:24,295\tDEBUG catalog.py:618 -- Created preprocessor <ray.rllib.models.preprocessors.NoPreprocessor object at 0x000001F16B70E6A0>: Box(-inf, inf, (25, 5), float32) -> (25, 5)\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:24,295\tWARNING deprecation.py:33 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:28:24,289\tDEBUG rollout_worker.py:494 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:28:24,289\tDEBUG rollout_worker.py:634 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x000001DC84098E20> (<TradingEnv instance>), policies {'default_policy': <ray.rllib.policy.policy_template.PPOTorchPolicy object at 0x000001DC8404ABB0>}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:24,334\tINFO torch_policy.py:109 -- TorchPolicy running on GPU.\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:26,977\tINFO rollout_worker.py:1114 -- Built policy map: {'default_policy': <ray.rllib.policy.policy_template.PPOTorchPolicy object at 0x000001F16B70E700>}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:26,977\tINFO rollout_worker.py:1115 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x000001F16B70E6A0>}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:26,977\tDEBUG rollout_worker.py:494 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:26,978\tINFO rollout_worker.py:526 -- Built filter map: {'default_policy': MeanStdFilter((25, 5), True, True, None, (n=0, mean_mean=0.0, mean_std=0.0), (n=0, mean_mean=0.0, mean_std=0.0))}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:26,978\tDEBUG rollout_worker.py:634 -- Created rollout worker with env None (None), policies {'default_policy': <ray.rllib.policy.policy_template.PPOTorchPolicy object at 0x000001F16B70E700>}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:26,983\tINFO trainable.py:100 -- Trainable.setup took 15.239 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:26,983\tWARNING util.py:47 -- Install gputil for GPU system monitoring.\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:28:26,991\tINFO rollout_worker.py:659 -- Generating sample batch of size 200\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:28:26,991\tDEBUG sampler.py:540 -- No episode horizon specified, assuming inf.\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:28:26,993\tINFO sampler.py:593 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 5), dtype=float32, min=0.0, max=100.0, mean=3.2)}}\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:28:26,993\tINFO sampler.py:595 -- Info return from env: {0: {'agent0': None}}\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:28:26,993\tWARNING deprecation.py:33 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:28:26,993\tINFO sampler.py:1058 -- Preprocessed obs: np.ndarray((25, 5), dtype=float32, min=0.0, max=100.0, mean=3.2)\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:28:26,994\tINFO sampler.py:1063 -- Filtered obs: np.ndarray((25, 5), dtype=float64, min=0.0, max=0.0, mean=0.0)\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:28:26,994\tINFO sampler.py:1334 -- Inputs to compute_actions():\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m \r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                                   'env_id': 0,\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                                   'info': None,\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                                   'obs': np.ndarray((25, 5), dtype=float64, min=0.0, max=0.0, mean=0.0),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                                   'prev_action': None,\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                                   'prev_reward': 0.0,\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                                   'rnn_state': None},\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'type': 'PolicyEvalData'}]}\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m \r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:28:27,006\tINFO sampler.py:1352 -- Outputs of compute_actions():\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m \r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                       [],\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=0.0, max=0.0, mean=0.0),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.693, max=-0.693, mean=-0.693),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.5, max=0.5, mean=0.5),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m \r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:28:27,356\tINFO simple_list_collector.py:672 -- Trajectory fragment after postprocess_trajectory():\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m \r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m { 'agent0': { 'data': { 'action_dist_inputs': np.ndarray((102, 2), dtype=float32, min=-0.007, max=0.004, mean=-0.003),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'action_logp': np.ndarray((102,), dtype=float32, min=-0.696, max=-0.69, mean=-0.693),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'actions': np.ndarray((102,), dtype=int64, min=0.0, max=1.0, mean=0.392),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'advantages': np.ndarray((102,), dtype=float32, min=-1.093, max=1.948, mean=0.1),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'agent_index': np.ndarray((102,), dtype=int32, min=0.0, max=0.0, mean=0.0),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'dones': np.ndarray((102,), dtype=bool, min=0.0, max=1.0, mean=0.01),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'eps_id': np.ndarray((102,), dtype=int32, min=425321682.0, max=425321682.0, mean=425321682.0),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'new_obs': np.ndarray((102, 25, 5), dtype=float32, min=-1.716, max=4.903, mean=0.64),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'obs': np.ndarray((102, 25, 5), dtype=float32, min=-1.715, max=4.903, mean=0.636),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'rewards': np.ndarray((102,), dtype=float32, min=-1.0, max=1.0, mean=-0.147),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'unroll_id': np.ndarray((102,), dtype=int32, min=0.0, max=0.0, mean=0.0),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'value_targets': np.ndarray((102,), dtype=float32, min=-1.0, max=1.0, mean=-0.147),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'vf_preds': np.ndarray((102,), dtype=float32, min=-1.173, max=0.272, mean=-0.247)},\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m               'type': 'SampleBatch'}}\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m \r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:28:27,710\tINFO rollout_worker.py:697 -- Completed sample batch:\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m \r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m { 'data': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-0.007, max=0.01, mean=-0.002),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m             'action_logp': np.ndarray((200,), dtype=float32, min=-0.697, max=-0.69, mean=-0.693),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m             'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.44),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m             'advantages': np.ndarray((200,), dtype=float32, min=-1.994, max=1.948, mean=0.116),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m             'agent_index': np.ndarray((200,), dtype=int32, min=0.0, max=0.0, mean=0.0),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m             'eps_id': np.ndarray((200,), dtype=int32, min=324757024.0, max=425321682.0, mean=376044999.58),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m             'obs': np.ndarray((200, 25, 5), dtype=float32, min=-6.014, max=4.903, mean=0.378),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m             'unroll_id': np.ndarray((200,), dtype=int32, min=0.0, max=1.0, mean=0.49),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m             'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=-0.1),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m             'vf_preds': np.ndarray((200,), dtype=float32, min=-1.173, max=1.127, mean=-0.216)},\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m   'type': 'SampleBatch'}\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m \r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:42,144\tINFO rollout_worker.py:837 -- Training on concatenated sample batches:\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m \r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m { 'count': 128,\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m   'policy_batches': { 'default_policy': { 'data': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-0.007, max=0.011, mean=-0.001),\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                                     'action_logp': np.ndarray((128,), dtype=float32, min=-0.697, max=-0.689, mean=-0.693),\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                                     'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.508),\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                                     'advantages': np.ndarray((128,), dtype=float32, min=-2.123, max=1.232, mean=-0.033),\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                                     'agent_index': np.ndarray((128,), dtype=int32, min=0.0, max=0.0, mean=0.0),\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                                     'eps_id': np.ndarray((128,), dtype=int32, min=106307593.0, max=1868635134.0, mean=966559258.289),\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                                     'obs': np.ndarray((128, 25, 5), dtype=float32, min=-7.467, max=1.856, mean=0.028),\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                                     'unroll_id': np.ndarray((128,), dtype=int32, min=0.0, max=54.0, mean=25.758),\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                                     'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=-0.023),\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                                     'vf_preds': np.ndarray((128,), dtype=float32, min=-0.577, max=1.23, mean=-0.139)},\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                           'type': 'SampleBatch'}},\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m   'type': 'MultiAgentBatch'}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m \r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:42,203\tDEBUG rollout_worker.py:863 -- Training out:\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m \r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m { 'default_policy': { 'learner_stats': { 'allreduce_latency': 0.0,\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                          'cur_kl_coeff': 0.2,\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                          'cur_lr': 8e-06,\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                          'entropy': 0.6931454539299011,\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                          'entropy_coeff': 0.01,\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                          'kl': -1.1237538677377756e-12,\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                          'policy_loss': 0.03276386111974716,\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                          'total_loss': 0.579954206943512,\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=-0.122, max=-0.122, mean=-0.122),\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                          'vf_loss': 1.108243703842163},\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                       'model': {}}}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m \r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:42,570\tDEBUG sgd.py:139 -- 0 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.5146761946380138, 'policy_loss': -0.007242381572723389, 'vf_loss': 1.057679183781147, 'vf_explained_var': -0.05847246, 'kl': 4.918197504984916e-05, 'entropy': 0.6930841226130724, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:43,043\tDEBUG sgd.py:139 -- 1 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.4877252848818898, 'policy_loss': -0.014276639267336577, 'vf_loss': 1.0177108086645603, 'vf_explained_var': -0.025578924, 'kl': 0.00036952485379515565, 'entropy': 0.6927378848195076, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:43,428\tDEBUG sgd.py:139 -- 2 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.4777683140709996, 'policy_loss': -0.021942706836853176, 'vf_loss': 1.0127995423972607, 'vf_explained_var': -0.02117242, 'kl': 0.001152601720605162, 'entropy': 0.6919263899326324, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:43,832\tDEBUG sgd.py:139 -- 3 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.4560449505224824, 'policy_loss': -0.04108385706786066, 'vf_loss': 1.0070093087852001, 'vf_explained_var': -0.015768155, 'kl': 0.0026411706312501337, 'entropy': 0.6904085967689753, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:44,221\tDEBUG sgd.py:139 -- 4 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.4367030137218535, 'policy_loss': -0.058839238074142486, 'vf_loss': 1.0027803238481283, 'vf_explained_var': -0.013285987, 'kl': 0.00515415878908243, 'entropy': 0.6878755763173103, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:44,621\tDEBUG sgd.py:139 -- 5 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.4206269849091768, 'policy_loss': -0.07528188798460178, 'vf_loss': 1.001884525641799, 'vf_explained_var': -0.011474032, 'kl': 0.009033386144437827, 'entropy': 0.684007415547967, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:45,031\tDEBUG sgd.py:139 -- 6 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.39207187574356794, 'policy_loss': -0.10395429970230907, 'vf_loss': 0.9996536523103714, 'vf_explained_var': -0.007704692, 'kl': 0.01490792166441679, 'entropy': 0.6782231796532869, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:45,432\tDEBUG sgd.py:139 -- 7 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.37555749667808414, 'policy_loss': -0.12098045606398955, 'vf_loss': 0.997406255453825, 'vf_explained_var': -0.00656073, 'kl': 0.022706994612235576, 'entropy': 0.6706568486988544, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:45,846\tDEBUG sgd.py:139 -- 8 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.35968976793810725, 'policy_loss': -0.1371436851331964, 'vf_loss': 0.9953487049788237, 'vf_explained_var': -0.0038830861, 'kl': 0.02902633708436042, 'entropy': 0.6646172031760216, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:46,233\tDEBUG sgd.py:139 -- 9 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.3548495154827833, 'policy_loss': -0.14197889051865786, 'vf_loss': 0.9940878357738256, 'vf_explained_var': -0.0029131584, 'kl': 0.03201283433008939, 'entropy': 0.6618069168180227, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:46,619\tDEBUG sgd.py:139 -- 10 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.35434366250410676, 'policy_loss': -0.14253867894876748, 'vf_loss': 0.9933826792985201, 'vf_explained_var': -0.0026661176, 'kl': 0.03395467915106565, 'entropy': 0.6599938329309225, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:47,003\tDEBUG sgd.py:139 -- 11 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.33665432315319777, 'policy_loss': -0.16019342036452144, 'vf_loss': 0.992367185652256, 'vf_explained_var': -0.001497468, 'kl': 0.036214899038895965, 'entropy': 0.6578822042793036, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:47,387\tDEBUG sgd.py:139 -- 12 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.3401020634919405, 'policy_loss': -0.15755244146566838, 'vf_loss': 0.9935729820281267, 'vf_explained_var': -0.0030428302, 'kl': 0.037190018221735954, 'entropy': 0.6569981053471565, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:47,758\tDEBUG sgd.py:139 -- 13 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.33480060240253806, 'policy_loss': -0.16187585284933448, 'vf_loss': 0.9911166056990623, 'vf_explained_var': -0.00014763512, 'kl': 0.03838580462615937, 'entropy': 0.6559011992067099, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:48,138\tDEBUG sgd.py:139 -- 14 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.3354346649721265, 'policy_loss': -0.16145948431221768, 'vf_loss': 0.9911786839365959, 'vf_explained_var': -0.0007440131, 'kl': 0.0392789636971429, 'entropy': 0.6550985146313906, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:48,567\tDEBUG sgd.py:139 -- 15 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.33278144523501396, 'policy_loss': -0.16437134484294802, 'vf_loss': 0.9912828113883734, 'vf_explained_var': -0.0011089649, 'kl': 0.04026708146557212, 'entropy': 0.6542038079351187, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:49,011\tDEBUG sgd.py:139 -- 16 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.32755941385403275, 'policy_loss': -0.1698188746231608, 'vf_loss': 0.9911166653037071, 'vf_explained_var': 9.8327175e-05, 'kl': 0.04174233437515795, 'entropy': 0.6528506316244602, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:49,426\tDEBUG sgd.py:139 -- 17 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.32726933155208826, 'policy_loss': -0.16965043573873118, 'vf_loss': 0.9897354859858751, 'vf_explained_var': 0.0008106362, 'kl': 0.042852057493291795, 'entropy': 0.6518396735191345, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:49,850\tDEBUG sgd.py:139 -- 18 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.31827023113146424, 'policy_loss': -0.17852413869695738, 'vf_loss': 0.9889835529029369, 'vf_explained_var': 0.001992371, 'kl': 0.04405079141724855, 'entropy': 0.6507563125342131, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:50,221\tDEBUG sgd.py:139 -- 19 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.31547310296446085, 'policy_loss': -0.1814153779996559, 'vf_loss': 0.98855359852314, 'vf_explained_var': 0.0024655499, 'kl': 0.04552887554746121, 'entropy': 0.6494092475622892, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:50,646\tDEBUG sgd.py:139 -- 20 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.31331324018538, 'policy_loss': -0.18406378582585603, 'vf_loss': 0.9889404755085707, 'vf_explained_var': 0.0018798411, 'kl': 0.04694063786882907, 'entropy': 0.648133710026741, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:51,075\tDEBUG sgd.py:139 -- 21 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.31881204014644027, 'policy_loss': -0.17890516039915383, 'vf_loss': 0.9892955180257559, 'vf_explained_var': 0.0018286426, 'kl': 0.047719721333123744, 'entropy': 0.6474507972598076, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:51,494\tDEBUG sgd.py:139 -- 22 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.31143842777237296, 'policy_loss': -0.18705893866717815, 'vf_loss': 0.9903192445635796, 'vf_explained_var': 0.00078947097, 'kl': 0.0490029965294525, 'entropy': 0.6462852470576763, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:51,873\tDEBUG sgd.py:139 -- 23 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.3096841345541179, 'policy_loss': -0.18780976359266788, 'vf_loss': 0.9876554533839226, 'vf_explained_var': 0.0022985227, 'kl': 0.050574096851050854, 'entropy': 0.6448644436895847, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:52,279\tDEBUG sgd.py:139 -- 24 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.308674484025687, 'policy_loss': -0.18943389668129385, 'vf_loss': 0.9882247447967529, 'vf_explained_var': 0.001727391, 'kl': 0.05215212400071323, 'entropy': 0.6434419322758913, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:52,671\tDEBUG sgd.py:139 -- 25 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.2998943733982742, 'policy_loss': -0.19853784935548902, 'vf_loss': 0.9883373267948627, 'vf_explained_var': 0.0029621515, 'kl': 0.05343329627066851, 'entropy': 0.6423102784901857, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:53,102\tDEBUG sgd.py:139 -- 26 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.3078407509601675, 'policy_loss': -0.19069556915201247, 'vf_loss': 0.9880069978535175, 'vf_explained_var': 0.0031241067, 'kl': 0.054722017142921686, 'entropy': 0.6411582622677088, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:53,498\tDEBUG sgd.py:139 -- 27 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.2905373377725482, 'policy_loss': -0.20771920320112258, 'vf_loss': 0.9870278891175985, 'vf_explained_var': 0.004554687, 'kl': 0.05572625086642802, 'entropy': 0.6402649637311697, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:53,888\tDEBUG sgd.py:139 -- 28 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.2877818781416863, 'policy_loss': -0.21089632681105286, 'vf_loss': 0.9871420804411173, 'vf_explained_var': 0.0038412362, 'kl': 0.05747074470855296, 'entropy': 0.638697899878025, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:54,488\tDEBUG sgd.py:139 -- 29 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.2, 'cur_lr': 8e-06, 'total_loss': 0.2832002036157064, 'policy_loss': -0.2149341448675841, 'vf_loss': 0.9853816982358694, 'vf_explained_var': 0.004196275, 'kl': 0.059079192811623216, 'entropy': 0.6372336521744728, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:28:54,509\tDEBUG trainer.py:546 -- synchronized filters: {'default_policy': MeanStdFilter((25, 5), True, True, None, (n=4037, mean_mean=90.65360445486996, mean_std=31.63645361164414), (n=0, mean_mean=0.0, mean_std=0.0))}\r\n",
      "2021-04-18 15:28:54,621\tWARNING trial_runner.py:420 -- Trial Runner checkpointing failed: [WinError 183] 当文件已存在时，无法创建该文件。: 'C:\\\\Users\\\\xichao.chen\\\\ray_results\\\\PPO\\\\.tmp_generator' -> 'C:\\\\Users\\\\xichao.chen\\\\ray_results\\\\PPO\\\\basic-variant-state-2021-04-18_15-28-06.json'\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:09,147\tDEBUG sgd.py:139 -- 0 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.28059606114402413, 'policy_loss': -0.21614205627702177, 'vf_loss': 0.9151388797909021, 'vf_explained_var': -0.021494899, 'kl': 0.14369916926384949, 'entropy': 0.3941071853041649, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:09,504\tDEBUG sgd.py:139 -- 1 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.22825677372748032, 'policy_loss': -0.24800901277922094, 'vf_loss': 0.8918838556855917, 'vf_explained_var': -0.016321331, 'kl': 0.11453771102242172, 'entropy': 0.4037453215569258, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:09,867\tDEBUG sgd.py:139 -- 2 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.23074187070596963, 'policy_loss': -0.2401702394708991, 'vf_loss': 0.8856845013797283, 'vf_explained_var': -0.006817071, 'kl': 0.10727738658897579, 'entropy': 0.4113353928551078, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:10,248\tDEBUG sgd.py:139 -- 3 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.22295098437462002, 'policy_loss': -0.24851531395688653, 'vf_loss': 0.8875363972038031, 'vf_explained_var': -0.009303303, 'kl': 0.10609273728914559, 'entropy': 0.4129730835556984, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:10,683\tDEBUG sgd.py:139 -- 4 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.2260750071145594, 'policy_loss': -0.242322604986839, 'vf_loss': 0.8808320965617895, 'vf_explained_var': 0.0044312626, 'kl': 0.10699060629121959, 'entropy': 0.41156274918466806, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:11,358\tDEBUG sgd.py:139 -- 5 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.21600921801291406, 'policy_loss': -0.24835682660341263, 'vf_loss': 0.8751603905111551, 'vf_explained_var': -0.0024410635, 'kl': 0.10317570064216852, 'entropy': 0.41668582148849964, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:11,990\tDEBUG sgd.py:139 -- 6 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.21309635462239385, 'policy_loss': -0.244384431745857, 'vf_loss': 0.8613261468708515, 'vf_explained_var': 0.016333131, 'kl': 0.10325954551808536, 'entropy': 0.4160152906551957, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:12,528\tDEBUG sgd.py:139 -- 7 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.21240299381315708, 'policy_loss': -0.2483936222270131, 'vf_loss': 0.8681987579911947, 'vf_explained_var': 0.00942713, 'kl': 0.10286681307479739, 'entropy': 0.41628122236579657, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:13,310\tDEBUG sgd.py:139 -- 8 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.2156362202949822, 'policy_loss': -0.24456814175937325, 'vf_loss': 0.8653239160776138, 'vf_explained_var': 0.017267207, 'kl': 0.10559271555393934, 'entropy': 0.4135408438742161, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:14,026\tDEBUG sgd.py:139 -- 9 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.20059245196171105, 'policy_loss': -0.2548146704211831, 'vf_loss': 0.8563149180263281, 'vf_explained_var': 0.019627897, 'kl': 0.10463263653218746, 'entropy': 0.41401330940425396, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:14,496\tDEBUG sgd.py:139 -- 10 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.21852400136413053, 'policy_loss': -0.24383245059289038, 'vf_loss': 0.8696207646280527, 'vf_explained_var': 0.013529522, 'kl': 0.10556952143087983, 'entropy': 0.4124790783971548, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:14,960\tDEBUG sgd.py:139 -- 11 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.2131360456114635, 'policy_loss': -0.24817235628142953, 'vf_loss': 0.8669427633285522, 'vf_explained_var': 0.0128325075, 'kl': 0.10653413948602974, 'entropy': 0.4123222604393959, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:15,387\tDEBUG sgd.py:139 -- 12 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.20745215541683137, 'policy_loss': -0.25139987375587225, 'vf_loss': 0.8628940936177969, 'vf_explained_var': 0.013433112, 'kl': 0.10513330739922822, 'entropy': 0.4135018130764365, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:15,843\tDEBUG sgd.py:139 -- 13 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.19850357936229557, 'policy_loss': -0.2545562026789412, 'vf_loss': 0.8500988408923149, 'vf_explained_var': 0.026449462, 'kl': 0.10708552552387118, 'entropy': 0.41152998711913824, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:16,336\tDEBUG sgd.py:139 -- 14 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.2214673738926649, 'policy_loss': -0.2430466718506068, 'vf_loss': 0.8741834163665771, 'vf_explained_var': 0.012399659, 'kl': 0.1051982946228236, 'entropy': 0.4137151362374425, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:16,830\tDEBUG sgd.py:139 -- 15 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.20841125503648072, 'policy_loss': -0.2524167844094336, 'vf_loss': 0.8655614443123341, 'vf_explained_var': 0.013321197, 'kl': 0.1071926481090486, 'entropy': 0.41104735992848873, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:17,269\tDEBUG sgd.py:139 -- 16 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.21412105741910636, 'policy_loss': -0.24631139310076833, 'vf_loss': 0.8642605282366276, 'vf_explained_var': 0.019589609, 'kl': 0.10801108251325786, 'entropy': 0.41011422500014305, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:17,716\tDEBUG sgd.py:139 -- 17 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.2098946113837883, 'policy_loss': -0.24843649519607425, 'vf_loss': 0.8606180995702744, 'vf_explained_var': 0.0208476, 'kl': 0.10711137228645384, 'entropy': 0.41113567166030407, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:18,147\tDEBUG sgd.py:139 -- 18 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.2072033014264889, 'policy_loss': -0.2502960213460028, 'vf_loss': 0.8589902278035879, 'vf_explained_var': 0.02197613, 'kl': 0.10703586391173303, 'entropy': 0.4106557108461857, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:18,578\tDEBUG sgd.py:139 -- 19 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.20446121401619166, 'policy_loss': -0.25306710856966674, 'vf_loss': 0.8583914469927549, 'vf_explained_var': 0.019533025, 'kl': 0.10810405481606722, 'entropy': 0.4098616801202297, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:19,009\tDEBUG sgd.py:139 -- 20 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.2115561586106196, 'policy_loss': -0.25085536180995405, 'vf_loss': 0.8675376772880554, 'vf_explained_var': 0.011805775, 'kl': 0.1091081197373569, 'entropy': 0.4089753860607743, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:19,457\tDEBUG sgd.py:139 -- 21 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.20980332628823817, 'policy_loss': -0.25006703031249344, 'vf_loss': 0.8624695781618357, 'vf_explained_var': 0.018340047, 'kl': 0.10910107591189444, 'entropy': 0.4094762681052089, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:19,947\tDEBUG sgd.py:139 -- 22 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.21331537840887904, 'policy_loss': -0.24958940595388412, 'vf_loss': 0.8683710731565952, 'vf_explained_var': 0.012756059, 'kl': 0.10937501373700798, 'entropy': 0.40932563692331314, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:20,388\tDEBUG sgd.py:139 -- 23 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.20363865769468248, 'policy_loss': -0.2507648877799511, 'vf_loss': 0.8532794360071421, 'vf_explained_var': 0.02510971, 'kl': 0.10629488690756261, 'entropy': 0.41246454417705536, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:20,807\tDEBUG sgd.py:139 -- 24 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.20416421652771533, 'policy_loss': -0.2527703191153705, 'vf_loss': 0.8576147872954607, 'vf_explained_var': 0.022428006, 'kl': 0.10744061949662864, 'entropy': 0.4105046782642603, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:21,339\tDEBUG sgd.py:139 -- 25 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.20479271782096475, 'policy_loss': -0.2521561507601291, 'vf_loss': 0.8566413689404726, 'vf_explained_var': 0.02420454, 'kl': 0.10907183610834181, 'entropy': 0.4093374628573656, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:22,020\tDEBUG sgd.py:139 -- 26 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.20021973608527333, 'policy_loss': -0.25626495433971286, 'vf_loss': 0.8559693191200495, 'vf_explained_var': 0.021777917, 'kl': 0.10864077298901975, 'entropy': 0.40922022704035044, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:22,577\tDEBUG sgd.py:139 -- 27 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.20079828333109617, 'policy_loss': -0.2574447365477681, 'vf_loss': 0.858492985367775, 'vf_explained_var': 0.018149031, 'kl': 0.11024511605501175, 'entropy': 0.40770076774060726, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:23,148\tDEBUG sgd.py:139 -- 28 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.19373126287246123, 'policy_loss': -0.25967996660619974, 'vf_loss': 0.8493333924561739, 'vf_explained_var': 0.026547985, 'kl': 0.10943345399573445, 'entropy': 0.4085515160113573, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:23,706\tDEBUG sgd.py:139 -- 29 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.30000000000000004, 'cur_lr': 0.0007000000000000001, 'total_loss': 0.203319443797227, 'policy_loss': -0.25306866178289056, 'vf_loss': 0.8569341525435448, 'vf_explained_var': 0.023533247, 'kl': 0.10678977146744728, 'entropy': 0.41159062646329403, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:23,733\tDEBUG trainer.py:546 -- synchronized filters: {'default_policy': MeanStdFilter((25, 5), True, True, None, (n=8068, mean_mean=92.79770906584456, mean_std=31.02227186232496), (n=0, mean_mean=0.0, mean_std=0.0))}\r\n",
      "2021-04-18 15:29:23,840\tWARNING trial_runner.py:420 -- Trial Runner checkpointing failed: [WinError 183] 当文件已存在时，无法创建该文件。: 'C:\\\\Users\\\\xichao.chen\\\\ray_results\\\\PPO\\\\.tmp_generator' -> 'C:\\\\Users\\\\xichao.chen\\\\ray_results\\\\PPO\\\\basic-variant-state-2021-04-18_15-28-06.json'\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:29:27,710\tINFO sampler.py:1058 -- Preprocessed obs: np.ndarray((25, 5), dtype=float32, min=-0.008, max=93.823, mean=50.967)\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:29:27,710\tINFO sampler.py:1063 -- Filtered obs: np.ndarray((25, 5), dtype=float64, min=-4.339, max=-0.073, mean=-1.559)\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:29:27,711\tWARNING deprecation.py:33 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:29:27,711\tINFO sampler.py:1334 -- Inputs to compute_actions():\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m \r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                                   'env_id': 0,\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                                   'info': { 'net_worth': 59568.66,\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                                             'step': 248},\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                                   'obs': np.ndarray((25, 5), dtype=float64, min=-4.339, max=-0.073, mean=-1.559),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                                   'prev_action': 0,\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                                   'prev_reward': 0.04873447054611546,\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                                   'rnn_state': []},\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'type': 'PolicyEvalData'}]}\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m \r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:29:27,715\tINFO sampler.py:1352 -- Outputs of compute_actions():\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m \r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                       [],\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                       { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-1.081, max=1.074, mean=-0.003),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-0.11, max=-0.11, mean=-0.11),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.896, max=0.896, mean=0.896),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.291, max=-0.291, mean=-0.291)})}\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m \r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:29:27,716\tINFO sampler.py:593 -- Raw obs from env: { 0: { 'agent0': np.ndarray((25, 5), dtype=float32, min=-0.008, max=93.024, mean=50.59)}}\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:29:27,716\tINFO sampler.py:595 -- Info return from env: {0: {'agent0': {'net_worth': 59568.66, 'step': 249}}}\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:29:27,837\tINFO simple_list_collector.py:672 -- Trajectory fragment after postprocess_trajectory():\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m \r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m { 'agent0': { 'data': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-1.271, max=1.266, mean=-0.003),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'action_logp': np.ndarray((200,), dtype=float32, min=-2.609, max=-0.076, mean=-0.27),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.135),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'advantages': np.ndarray((200,), dtype=float32, min=-1.7, max=1.342, mean=0.386),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'agent_index': np.ndarray((200,), dtype=int32, min=0.0, max=0.0, mean=0.0),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'eps_id': np.ndarray((200,), dtype=int32, min=6107006.0, max=6107006.0, mean=6107006.0),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'new_obs': np.ndarray((200, 25, 5), dtype=float32, min=-4.501, max=1.543, mean=-0.54),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'obs': np.ndarray((200, 25, 5), dtype=float32, min=-4.475, max=1.543, mean=-0.531),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=0.66),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'unroll_id': np.ndarray((200,), dtype=int32, min=111.0, max=111.0, mean=111.0),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=0.66),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m                         'vf_preds': np.ndarray((200,), dtype=float32, min=-0.342, max=0.7, mean=0.274)},\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m               'type': 'SampleBatch'}}\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m \r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:29:27,839\tINFO rollout_worker.py:697 -- Completed sample batch:\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m \r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m { 'data': { 'action_dist_inputs': np.ndarray((200, 2), dtype=float32, min=-1.271, max=1.266, mean=-0.003),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m             'action_logp': np.ndarray((200,), dtype=float32, min=-2.609, max=-0.076, mean=-0.27),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m             'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.135),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m             'advantages': np.ndarray((200,), dtype=float32, min=-1.7, max=1.342, mean=0.386),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m             'agent_index': np.ndarray((200,), dtype=int32, min=0.0, max=0.0, mean=0.0),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m             'eps_id': np.ndarray((200,), dtype=int32, min=6107006.0, max=6107006.0, mean=6107006.0),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m             'obs': np.ndarray((200, 25, 5), dtype=float32, min=-4.475, max=1.543, mean=-0.531),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m             'unroll_id': np.ndarray((200,), dtype=int32, min=111.0, max=111.0, mean=111.0),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m             'value_targets': np.ndarray((200,), dtype=float32, min=-1.0, max=1.0, mean=0.66),\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m             'vf_preds': np.ndarray((200,), dtype=float32, min=-0.342, max=0.7, mean=0.274)},\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m   'type': 'SampleBatch'}\r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m \r\n",
      "\u001B[2m\u001B[36m(pid=11900)\u001B[0m 2021-04-18 15:29:27,842\tINFO rollout_worker.py:659 -- Generating sample batch of size 200\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:41,796\tDEBUG sgd.py:139 -- 0 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.22203714458737522, 'policy_loss': -0.10269884002627805, 'vf_loss': 0.6095799021422863, 'vf_explained_var': 0.022722032, 'kl': 0.04949126666588416, 'entropy': 0.2325033713132143, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:42,217\tINFO rollout_worker.py:837 -- Training on concatenated sample batches:\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m \r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m { 'count': 128,\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m   'policy_batches': { 'default_policy': { 'data': { 'action_dist_inputs': np.ndarray((128, 2), dtype=float32, min=-1.274, max=1.269, mean=-0.001),\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                                     'action_logp': np.ndarray((128,), dtype=float32, min=-2.6, max=-0.076, mean=-0.336),\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                                     'actions': np.ndarray((128,), dtype=int64, min=0.0, max=1.0, mean=0.367),\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                                     'advantages': np.ndarray((128,), dtype=float32, min=-2.614, max=1.381, mean=0.058),\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                                     'agent_index': np.ndarray((128,), dtype=int32, min=0.0, max=0.0, mean=0.0),\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                                     'eps_id': np.ndarray((128,), dtype=int32, min=6107006.0, max=1866899227.0, mean=770769836.375),\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                                     'obs': np.ndarray((128, 25, 5), dtype=float32, min=-5.229, max=1.909, mean=-0.177),\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                                     'unroll_id': np.ndarray((128,), dtype=int32, min=106.0, max=138.0, mean=120.93),\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                                     'value_targets': np.ndarray((128,), dtype=float32, min=-1.0, max=1.0, mean=0.688),\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                                     'vf_preds': np.ndarray((128,), dtype=float32, min=-0.485, max=0.698, mean=0.251)},\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                           'type': 'SampleBatch'}},\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m   'type': 'MultiAgentBatch'}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m \r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:42,231\tDEBUG rollout_worker.py:863 -- Training out:\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m \r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m { 'default_policy': { 'learner_stats': { 'allreduce_latency': 0.0,\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                          'cur_kl_coeff': 0.45000000000000007,\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                          'cur_lr': 0.00030000000000000003,\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                          'entropy': 0.2822519540786743,\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                          'entropy_coeff': 0.01,\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                          'kl': 0.03565391153097153,\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                          'policy_loss': -0.16651315987110138,\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                          'total_loss': 0.07017754763364792,\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                          'vf_explained_var': np.ndarray((1,), dtype=float32, min=0.165, max=0.165, mean=0.165),\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                                          'vf_loss': 0.4469379484653473},\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m                       'model': {}}}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m \r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:42,231\tDEBUG sgd.py:139 -- 1 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.1444957279600203, 'policy_loss': -0.1183327334583737, 'vf_loss': 0.5045867078006268, 'vf_explained_var': 0.14460298, 'kl': 0.029335301864193752, 'entropy': 0.26657761028036475, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:42,795\tDEBUG sgd.py:139 -- 2 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.13616601331159472, 'policy_loss': -0.11873581603867933, 'vf_loss': 0.491314422339201, 'vf_explained_var': 0.16963485, 'kl': 0.02664721483597532, 'entropy': 0.27466303994879127, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:43,275\tDEBUG sgd.py:139 -- 3 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.12087248411262408, 'policy_loss': -0.12589040724560618, 'vf_loss': 0.4753993134945631, 'vf_explained_var': 0.17881754, 'kl': 0.026257643767166883, 'entropy': 0.2752704918384552, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:43,728\tDEBUG sgd.py:139 -- 4 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.13013156404485926, 'policy_loss': -0.11990691698156297, 'vf_loss': 0.48128784634172916, 'vf_explained_var': 0.18376411, 'kl': 0.026970628823619336, 'entropy': 0.274222026579082, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:44,212\tDEBUG sgd.py:139 -- 5 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.1233418284682557, 'policy_loss': -0.12438233767170459, 'vf_loss': 0.4757150476798415, 'vf_explained_var': 0.18614586, 'kl': 0.027978389523923397, 'entropy': 0.27236308716237545, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:44,711\tDEBUG sgd.py:139 -- 6 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.1270812979200855, 'policy_loss': -0.12318876024801284, 'vf_loss': 0.4800141444429755, 'vf_explained_var': 0.18920065, 'kl': 0.028856775141321123, 'entropy': 0.27225620578974485, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:45,160\tDEBUG sgd.py:139 -- 7 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.12279409181792289, 'policy_loss': -0.12439460243331268, 'vf_loss': 0.4725888967514038, 'vf_explained_var': 0.18937284, 'kl': 0.03014193329727277, 'entropy': 0.26696215383708477, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:45,604\tDEBUG sgd.py:139 -- 8 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.13220784271834418, 'policy_loss': -0.12083154122228734, 'vf_loss': 0.48198976181447506, 'vf_explained_var': 0.18484533, 'kl': 0.032714561850298196, 'entropy': 0.26770517975091934, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:46,045\tDEBUG sgd.py:139 -- 9 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.12801493669394404, 'policy_loss': -0.12195797683671117, 'vf_loss': 0.4769176943227649, 'vf_explained_var': 0.19662271, 'kl': 0.03147248842287809, 'entropy': 0.2648555696941912, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:46,452\tDEBUG sgd.py:139 -- 10 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.12078358797589317, 'policy_loss': -0.12993296576314606, 'vf_loss': 0.47824423387646675, 'vf_explained_var': 0.17918582, 'kl': 0.0316376420087181, 'entropy': 0.2642496800981462, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:46,877\tDEBUG sgd.py:139 -- 11 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.12088582455180585, 'policy_loss': -0.12612401851220056, 'vf_loss': 0.47236037254333496, 'vf_explained_var': 0.19238321, 'kl': 0.029993837291840464, 'entropy': 0.2667573862709105, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:47,410\tDEBUG sgd.py:139 -- 12 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.1268041426083073, 'policy_loss': -0.12266761669889092, 'vf_loss': 0.4782227072864771, 'vf_explained_var': 0.19277054, 'kl': 0.029014837695285678, 'entropy': 0.26962711615487933, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:47,857\tDEBUG sgd.py:139 -- 13 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.132989167352207, 'policy_loss': -0.11885300304857083, 'vf_loss': 0.4810951976105571, 'vf_explained_var': 0.18709503, 'kl': 0.03104774106759578, 'entropy': 0.2676911475136876, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:48,340\tDEBUG sgd.py:139 -- 14 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.1371511573670432, 'policy_loss': -0.11833274678792804, 'vf_loss': 0.48798801004886627, 'vf_explained_var': 0.18377519, 'kl': 0.03140790102770552, 'entropy': 0.2643659799359739, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:48,781\tDEBUG sgd.py:139 -- 15 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.12105121748754755, 'policy_loss': -0.1270920893875882, 'vf_loss': 0.47380616795271635, 'vf_explained_var': 0.19061305, 'kl': 0.03091221966315061, 'entropy': 0.26702732779085636, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:49,244\tDEBUG sgd.py:139 -- 16 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.11705883650574833, 'policy_loss': -0.12927267051418312, 'vf_loss': 0.4708748022094369, 'vf_explained_var': 0.19122636, 'kl': 0.030164245108608156, 'entropy': 0.26798037672415376, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:49,720\tDEBUG sgd.py:139 -- 17 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.11063511716201901, 'policy_loss': -0.13228003366384655, 'vf_loss': 0.463347602635622, 'vf_explained_var': 0.19199318, 'kl': 0.030895563948433846, 'entropy': 0.266165261156857, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:50,213\tDEBUG sgd.py:139 -- 18 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.11569182260427624, 'policy_loss': -0.1283544551115483, 'vf_loss': 0.4661993719637394, 'vf_explained_var': 0.1883791, 'kl': 0.030285757267847657, 'entropy': 0.268199912738055, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:50,892\tDEBUG sgd.py:139 -- 19 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.1290891095995903, 'policy_loss': -0.11981879989616573, 'vf_loss': 0.4765979601070285, 'vf_explained_var': 0.19648278, 'kl': 0.029604759823996574, 'entropy': 0.2713206931948662, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:51,508\tDEBUG sgd.py:139 -- 20 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.12348642223514616, 'policy_loss': -0.12702822929713875, 'vf_loss': 0.47777625173330307, 'vf_explained_var': 0.18755187, 'kl': 0.03176167979836464, 'entropy': 0.26662278501316905, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:51,930\tDEBUG sgd.py:139 -- 21 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.12399530783295631, 'policy_loss': -0.12513961759395897, 'vf_loss': 0.4719284549355507, 'vf_explained_var': 0.19646925, 'kl': 0.03511904482729733, 'entropy': 0.26328734774142504, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:52,431\tDEBUG sgd.py:139 -- 22 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.11705498863011599, 'policy_loss': -0.12860330467810854, 'vf_loss': 0.4690906717441976, 'vf_explained_var': 0.20934135, 'kl': 0.030706295801792294, 'entropy': 0.2704873587936163, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:52,889\tDEBUG sgd.py:139 -- 23 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.11694439977873117, 'policy_loss': -0.1278917429735884, 'vf_loss': 0.467393153347075, 'vf_explained_var': 0.2027435, 'kl': 0.030783162801526487, 'entropy': 0.2712860405445099, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:53,320\tDEBUG sgd.py:139 -- 24 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.11342919711023569, 'policy_loss': -0.13052986341062933, 'vf_loss': 0.46525082644075155, 'vf_explained_var': 0.20333613, 'kl': 0.031172043236438185, 'entropy': 0.26937679573893547, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:53,746\tDEBUG sgd.py:139 -- 25 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.11508539150236174, 'policy_loss': -0.13379469770006835, 'vf_loss': 0.4745249627158046, 'vf_explained_var': 0.18797812, 'kl': 0.03174747200682759, 'entropy': 0.26687500160187483, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:54,173\tDEBUG sgd.py:139 -- 26 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.12290978088276461, 'policy_loss': -0.12488459082669578, 'vf_loss': 0.4723426951095462, 'vf_explained_var': 0.20255816, 'kl': 0.03181656188098714, 'entropy': 0.26944261183962226, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:54,614\tDEBUG sgd.py:139 -- 27 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.12340112740639597, 'policy_loss': -0.12396812275983393, 'vf_loss': 0.46802538353949785, 'vf_explained_var': 0.20170277, 'kl': 0.03549685498001054, 'entropy': 0.2617025803774595, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:55,068\tDEBUG sgd.py:139 -- 28 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.10685941588599235, 'policy_loss': -0.13471472426317632, 'vf_loss': 0.4628505129367113, 'vf_explained_var': 0.19949967, 'kl': 0.02864970057271421, 'entropy': 0.2743481146171689, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:55,658\tDEBUG sgd.py:139 -- 29 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.45000000000000007, 'cur_lr': 0.00030000000000000003, 'total_loss': 0.1118882410810329, 'policy_loss': -0.130919169052504, 'vf_loss': 0.460341970436275, 'vf_explained_var': 0.21328866, 'kl': 0.03401086694793776, 'entropy': 0.2668465944007039, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:29:55,679\tDEBUG trainer.py:546 -- synchronized filters: {'default_policy': MeanStdFilter((25, 5), True, True, None, (n=12081, mean_mean=91.24937917736538, mean_std=30.793659593050933), (n=0, mean_mean=0.0, mean_std=0.0))}\r\n",
      "2021-04-18 15:29:55,749\tWARNING trial_runner.py:420 -- Trial Runner checkpointing failed: [WinError 183] 当文件已存在时，无法创建该文件。: 'C:\\\\Users\\\\xichao.chen\\\\ray_results\\\\PPO\\\\.tmp_generator' -> 'C:\\\\Users\\\\xichao.chen\\\\ray_results\\\\PPO\\\\basic-variant-state-2021-04-18_15-28-06.json'\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:14,554\tDEBUG sgd.py:139 -- 0 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.11944468098226935, 'policy_loss': -0.08645392284961417, 'vf_loss': 0.38085095351561904, 'vf_explained_var': 0.09686216, 'kl': 0.0257867804821703, 'entropy': 0.193294957280159, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:14,969\tDEBUG sgd.py:139 -- 1 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.0783139169216156, 'policy_loss': -0.10387809120584279, 'vf_loss': 0.34486342826858163, 'vf_explained_var': 0.12991211, 'kl': 0.017487898527178913, 'entropy': 0.20440386794507504, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:15,407\tDEBUG sgd.py:139 -- 2 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.07406505476683378, 'policy_loss': -0.10425130266230553, 'vf_loss': 0.3374903416261077, 'vf_explained_var': 0.14749226, 'kl': 0.017289494164288044, 'entropy': 0.20992203103378415, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:15,845\tDEBUG sgd.py:139 -- 3 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.054760837520007044, 'policy_loss': -0.12203778111143038, 'vf_loss': 0.3349568257108331, 'vf_explained_var': 0.15356067, 'kl': 0.016982065426418558, 'entropy': 0.21426875470206141, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:16,343\tDEBUG sgd.py:139 -- 4 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.06173398019745946, 'policy_loss': -0.11695314719690941, 'vf_loss': 0.3378578182309866, 'vf_explained_var': 0.13759553, 'kl': 0.01756601652596146, 'entropy': 0.20988456951454282, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:16,832\tDEBUG sgd.py:139 -- 5 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.06904048850992694, 'policy_loss': -0.10790434182854369, 'vf_loss': 0.3343210988678038, 'vf_explained_var': 0.16152999, 'kl': 0.01764161983737722, 'entropy': 0.2123812553472817, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:17,330\tDEBUG sgd.py:139 -- 6 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.06327395106200129, 'policy_loss': -0.11200381472008303, 'vf_loss': 0.3309910036623478, 'vf_explained_var': 0.1620242, 'kl': 0.01763212081277743, 'entropy': 0.21194148669019341, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:17,764\tDEBUG sgd.py:139 -- 7 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.06188205737271346, 'policy_loss': -0.11335368864820339, 'vf_loss': 0.33100785687565804, 'vf_explained_var': 0.16307442, 'kl': 0.017554323392687365, 'entropy': 0.21173472609370947, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:18,192\tDEBUG sgd.py:139 -- 8 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.06818807023228146, 'policy_loss': -0.1097017897409387, 'vf_loss': 0.3346918672323227, 'vf_explained_var': 0.15635926, 'kl': 0.018715218699071556, 'entropy': 0.20888454653322697, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:18,733\tDEBUG sgd.py:139 -- 9 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.05838272190885618, 'policy_loss': -0.11480044169002213, 'vf_loss': 0.32666349317878485, 'vf_explained_var': 0.16380186, 'kl': 0.01769996582879685, 'entropy': 0.20960589032620192, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:19,326\tDEBUG sgd.py:139 -- 10 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.07027882276452146, 'policy_loss': -0.1054018237919081, 'vf_loss': 0.3323096977546811, 'vf_explained_var': 0.16076463, 'kl': 0.017243740207049996, 'entropy': 0.2113724760711193, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:19,897\tDEBUG sgd.py:139 -- 11 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.05984912341227755, 'policy_loss': -0.11356803591479547, 'vf_loss': 0.3266743738204241, 'vf_explained_var': 0.17209136, 'kl': 0.018042706564301625, 'entropy': 0.20988523308187723, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:20,332\tDEBUG sgd.py:139 -- 12 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.06305334711214527, 'policy_loss': -0.11322658939752728, 'vf_loss': 0.33192503172904253, 'vf_explained_var': 0.16854264, 'kl': 0.018406806571874768, 'entropy': 0.21071710297837853, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:20,787\tDEBUG sgd.py:139 -- 13 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.06314237890182994, 'policy_loss': -0.11043188406620175, 'vf_loss': 0.3271443606354296, 'vf_explained_var': 0.17087168, 'kl': 0.017914072494022548, 'entropy': 0.20899165840819478, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:21,204\tDEBUG sgd.py:139 -- 14 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.059771628642920405, 'policy_loss': -0.11576854673330672, 'vf_loss': 0.3313326290808618, 'vf_explained_var': 0.15399757, 'kl': 0.0177444537403062, 'entropy': 0.21036460623145103, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:21,623\tDEBUG sgd.py:139 -- 15 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.06328870629658923, 'policy_loss': -0.11036913195857778, 'vf_loss': 0.3273554900661111, 'vf_explained_var': 0.17892101, 'kl': 0.017883362394059077, 'entropy': 0.20911774085834622, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:22,056\tDEBUG sgd.py:139 -- 16 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.06698163205874152, 'policy_loss': -0.10865045466925949, 'vf_loss': 0.3309411481022835, 'vf_explained_var': 0.16403985, 'kl': 0.018124114780221134, 'entropy': 0.20722670713439584, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:22,472\tDEBUG sgd.py:139 -- 17 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.05465451581403613, 'policy_loss': -0.11762881116010249, 'vf_loss': 0.3237485787831247, 'vf_explained_var': 0.14770012, 'kl': 0.018515055999159813, 'entropy': 0.20886211935430765, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:22,900\tDEBUG sgd.py:139 -- 18 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.05876986187649891, 'policy_loss': -0.11527952598407865, 'vf_loss': 0.3278384702280164, 'vf_explained_var': 0.1721999, 'kl': 0.018138082406949252, 'entropy': 0.21130531281232834, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:23,329\tDEBUG sgd.py:139 -- 19 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.060387453821022063, 'policy_loss': -0.1129058810765855, 'vf_loss': 0.32577539421617985, 'vf_explained_var': 0.17150405, 'kl': 0.01852452286402695, 'entropy': 0.2098413179628551, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:23,769\tDEBUG sgd.py:139 -- 20 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.06787247664760798, 'policy_loss': -0.10780053789494559, 'vf_loss': 0.33043840155005455, 'vf_explained_var': 0.17008746, 'kl': 0.01859454216901213, 'entropy': 0.2097500809468329, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:24,242\tDEBUG sgd.py:139 -- 21 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.05534761940361932, 'policy_loss': -0.11811779456911609, 'vf_loss': 0.32669660821557045, 'vf_explained_var': 0.17101353, 'kl': 0.018105111113982275, 'entropy': 0.21038369555026293, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:24,722\tDEBUG sgd.py:139 -- 22 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.06316707434598356, 'policy_loss': -0.11259717465145513, 'vf_loss': 0.33083577547222376, 'vf_explained_var': 0.16726741, 'kl': 0.018434056342812255, 'entropy': 0.20966262184083462, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:25,237\tDEBUG sgd.py:139 -- 23 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.059144085040315986, 'policy_loss': -0.1137052095727995, 'vf_loss': 0.3246910492889583, 'vf_explained_var': 0.17974192, 'kl': 0.018648644210770726, 'entropy': 0.20840653218328953, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:25,942\tDEBUG sgd.py:139 -- 24 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.06144396006129682, 'policy_loss': -0.11158752790652215, 'vf_loss': 0.32522273529320955, 'vf_explained_var': 0.17152444, 'kl': 0.018518618104280904, 'entropy': 0.20799447363242507, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:26,419\tDEBUG sgd.py:139 -- 25 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.055840696033556014, 'policy_loss': -0.12052639410831034, 'vf_loss': 0.33006548834964633, 'vf_explained_var': 0.16415009, 'kl': 0.01982023095479235, 'entropy': 0.20443103229627013, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:26,894\tDEBUG sgd.py:139 -- 26 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.06443013183888979, 'policy_loss': -0.10887702193576843, 'vf_loss': 0.3258206653408706, 'vf_explained_var': 0.18457, 'kl': 0.018489253445295617, 'entropy': 0.20834246324375272, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:27,499\tDEBUG sgd.py:139 -- 27 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.059717270167311653, 'policy_loss': -0.11433648323873058, 'vf_loss': 0.3270467487163842, 'vf_explained_var': 0.16660061, 'kl': 0.0187261005921755, 'entropy': 0.21097396640107036, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:27,985\tDEBUG sgd.py:139 -- 28 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.050902215152746066, 'policy_loss': -0.12062586442334577, 'vf_loss': 0.323018170427531, 'vf_explained_var': 0.17439097, 'kl': 0.01795163005590439, 'entropy': 0.2098356205970049, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:28,472\tDEBUG sgd.py:139 -- 29 {'allreduce_latency': 0.0, 'cur_kl_coeff': 0.675, 'cur_lr': 9.800000000000001e-05, 'total_loss': 0.06555486901197582, 'policy_loss': -0.10889514663722366, 'vf_loss': 0.3285615839995444, 'vf_explained_var': 0.16941723, 'kl': 0.01815431984141469, 'entropy': 0.20849454449489713, 'entropy_coeff': 0.01}\r\n",
      "\u001B[2m\u001B[36m(pid=14992)\u001B[0m 2021-04-18 15:30:28,487\tDEBUG trainer.py:546 -- synchronized filters: {'default_policy': MeanStdFilter((25, 5), True, True, None, (n=16087, mean_mean=88.67558091876634, mean_std=30.84224224716542), (n=0, mean_mean=0.0, mean_std=0.0))}\r\n",
      "2021-04-18 15:30:28,557\tWARNING trial_runner.py:420 -- Trial Runner checkpointing failed: [WinError 183] 当文件已存在时，无法创建该文件。: 'C:\\\\Users\\\\xichao.chen\\\\ray_results\\\\PPO\\\\.tmp_generator' -> 'C:\\\\Users\\\\xichao.chen\\\\ray_results\\\\PPO\\\\basic-variant-state-2021-04-18_15-28-06.json'\n",
      "2021-04-18 15:30:28,612\tWARNING tune.py:429 -- Trial Runner checkpointing failed: [WinError 183] 当文件已存在时，无法创建该文件。: 'C:\\\\Users\\\\xichao.chen\\\\ray_results\\\\PPO\\\\.tmp_generator' -> 'C:\\\\Users\\\\xichao.chen\\\\ray_results\\\\PPO\\\\basic-variant-state-2021-04-18_15-28-06.json'\n",
      "2021-04-18 15:30:28,989\tINFO tune.py:450 -- Total run time: 149.43 seconds (142.38 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 6.8/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 0/1 GPUs, 0.0/5.81 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: C:\\Users\\xichao.chen\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                </th><th>status  </th><th>loc  </th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_TradingEnv_9e021_00000</td><td>RUNNING </td><td>     </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TradingEnv_9e021_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-18_15-28-54\n",
      "  done: false\n",
      "  episode_len_mean: 108.44444444444444\n",
      "  episode_reward_max: 9.401380095178766\n",
      "  episode_reward_mean: -1.2165733775363325\n",
      "  episode_reward_min: -15.909444707195874\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 36\n",
      "  experiment_id: e891a448727c43bf9bb3603c0fdcc25c\n",
      "  hostname: LAPTOP-20JJPQJR\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 8.0e-06\n",
      "        entropy: 0.6372336521744728\n",
      "        entropy_coeff: 0.01\n",
      "        kl: 0.059079192811623216\n",
      "        policy_loss: -0.2149341448675841\n",
      "        total_loss: 0.2832002036157064\n",
      "        vf_explained_var: 0.004196275025606155\n",
      "        vf_loss: 0.9853816982358694\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.30.36\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.6275\n",
      "    ram_util_percent: 58.5825\n",
      "  pid: 14992\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05984520858539636\n",
      "    mean_env_wait_ms: 0.9710366831395487\n",
      "    mean_inference_ms: 2.4819924693738065\n",
      "    mean_raw_obs_processing_ms: 0.2550778464060133\n",
      "  time_since_restore: 27.51696491241455\n",
      "  time_this_iter_s: 27.51696491241455\n",
      "  time_total_s: 27.51696491241455\n",
      "  timers:\n",
      "    learn_throughput: 323.92\n",
      "    learn_time_ms: 12348.737\n",
      "    sample_throughput: 264.035\n",
      "    sample_time_ms: 15149.492\n",
      "    update_time_ms: 5.729\n",
      "  timestamp: 1618730934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 9e021_00000\n",
      "  \n",
      "Result for PPO_TradingEnv_9e021_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-18_15-29-23\n",
      "  done: false\n",
      "  episode_len_mean: 119.35820895522389\n",
      "  episode_reward_max: 53.40426973683584\n",
      "  episode_reward_mean: 11.83392982817052\n",
      "  episode_reward_min: -15.909444707195874\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 67\n",
      "  experiment_id: e891a448727c43bf9bb3603c0fdcc25c\n",
      "  hostname: LAPTOP-20JJPQJR\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.30000000000000004\n",
      "        cur_lr: 0.0007000000000000001\n",
      "        entropy: 0.41159062646329403\n",
      "        entropy_coeff: 0.01\n",
      "        kl: 0.10678977146744728\n",
      "        policy_loss: -0.25306866178289056\n",
      "        total_loss: 0.203319443797227\n",
      "        vf_explained_var: 0.023533247411251068\n",
      "        vf_loss: 0.8569341525435448\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.30.36\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.890243902439025\n",
      "    ram_util_percent: 58.78048780487805\n",
      "  pid: 14992\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05872274802036598\n",
      "    mean_env_wait_ms: 0.9500985119934972\n",
      "    mean_inference_ms: 2.449515206120114\n",
      "    mean_raw_obs_processing_ms: 0.253626843767891\n",
      "  time_since_restore: 56.63674306869507\n",
      "  time_this_iter_s: 29.119778156280518\n",
      "  time_total_s: 56.63674306869507\n",
      "  timers:\n",
      "    learn_throughput: 293.372\n",
      "    learn_time_ms: 13634.555\n",
      "    sample_throughput: 272.797\n",
      "    sample_time_ms: 14662.904\n",
      "    update_time_ms: 5.859\n",
      "  timestamp: 1618730963\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 9e021_00000\n",
      "  \n",
      "Result for PPO_TradingEnv_9e021_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-18_15-29-55\n",
      "  done: false\n",
      "  episode_len_mean: 146.9875\n",
      "  episode_reward_max: 221.0738439317575\n",
      "  episode_reward_mean: 30.76112316413445\n",
      "  episode_reward_min: -15.909444707195874\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 80\n",
      "  experiment_id: e891a448727c43bf9bb3603c0fdcc25c\n",
      "  hostname: LAPTOP-20JJPQJR\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.45000000000000007\n",
      "        cur_lr: 0.00030000000000000003\n",
      "        entropy: 0.2668465944007039\n",
      "        entropy_coeff: 0.01\n",
      "        kl: 0.03401086694793776\n",
      "        policy_loss: -0.130919169052504\n",
      "        total_loss: 0.1118882410810329\n",
      "        vf_explained_var: 0.21328866481781006\n",
      "        vf_loss: 0.460341970436275\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.30.36\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.12826086956522\n",
      "    ram_util_percent: 58.408695652173925\n",
      "  pid: 14992\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05916884967617322\n",
      "    mean_env_wait_ms: 0.9484636189810672\n",
      "    mean_inference_ms: 2.471551980554474\n",
      "    mean_raw_obs_processing_ms: 0.2608769251979818\n",
      "  time_since_restore: 88.47776985168457\n",
      "  time_this_iter_s: 31.841026782989502\n",
      "  time_total_s: 88.47776985168457\n",
      "  timers:\n",
      "    learn_throughput: 288.711\n",
      "    learn_time_ms: 13854.699\n",
      "    sample_throughput: 256.111\n",
      "    sample_time_ms: 15618.217\n",
      "    update_time_ms: 5.555\n",
      "  timestamp: 1618730995\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 9e021_00000\n",
      "  \n",
      "Result for PPO_TradingEnv_9e021_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-18_15-30-28\n",
      "  done: true\n",
      "  episode_len_mean: 184.72093023255815\n",
      "  episode_reward_max: 531.033620210002\n",
      "  episode_reward_mean: 52.9810219151239\n",
      "  episode_reward_min: -15.909444707195874\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 86\n",
      "  experiment_id: e891a448727c43bf9bb3603c0fdcc25c\n",
      "  hostname: LAPTOP-20JJPQJR\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.675\n",
      "        cur_lr: 9.800000000000001e-05\n",
      "        entropy: 0.20849454449489713\n",
      "        entropy_coeff: 0.01\n",
      "        kl: 0.01815431984141469\n",
      "        policy_loss: -0.10889514663722366\n",
      "        total_loss: 0.06555486901197582\n",
      "        vf_explained_var: 0.16941723227500916\n",
      "        vf_loss: 0.3285615839995444\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.30.36\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.91489361702128\n",
      "    ram_util_percent: 58.21276595744684\n",
      "  pid: 14992\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05957030505539639\n",
      "    mean_env_wait_ms: 0.9487220844405064\n",
      "    mean_inference_ms: 2.487903642395281\n",
      "    mean_raw_obs_processing_ms: 0.2660288654732252\n",
      "  time_since_restore: 121.21757507324219\n",
      "  time_this_iter_s: 32.73980522155762\n",
      "  time_total_s: 121.21757507324219\n",
      "  timers:\n",
      "    learn_throughput: 286.234\n",
      "    learn_time_ms: 13974.586\n",
      "    sample_throughput: 245.223\n",
      "    sample_time_ms: 16311.653\n",
      "    update_time_ms: 5.185\n",
      "  timestamp: 1618731028\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 9e021_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.3/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 0/1 GPUs, 0.0/5.81 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: C:\\Users\\xichao.chen\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_TradingEnv_9e021_00000</td><td>RUNNING </td><td>192.168.30.36:14992</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          27.517</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-1.21657</td><td style=\"text-align: right;\">             9.40138</td><td style=\"text-align: right;\">            -15.9094</td><td style=\"text-align: right;\">           108.444</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.3/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 0/1 GPUs, 0.0/5.81 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: C:\\Users\\xichao.chen\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_TradingEnv_9e021_00000</td><td>RUNNING </td><td>192.168.30.36:14992</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         56.6367</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\"> 11.8339</td><td style=\"text-align: right;\">             53.4043</td><td style=\"text-align: right;\">            -15.9094</td><td style=\"text-align: right;\">           119.358</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.2/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 0/1 GPUs, 0.0/5.81 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: C:\\Users\\xichao.chen\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_TradingEnv_9e021_00000</td><td>RUNNING </td><td>192.168.30.36:14992</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         88.4778</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> 30.7611</td><td style=\"text-align: right;\">             221.074</td><td style=\"text-align: right;\">            -15.9094</td><td style=\"text-align: right;\">           146.988</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.2/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 0/1 GPUs, 0.0/5.81 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: C:\\Users\\xichao.chen\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_TradingEnv_9e021_00000</td><td>RUNNING </td><td>192.168.30.36:14992</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         121.218</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">  52.981</td><td style=\"text-align: right;\">             531.034</td><td style=\"text-align: right;\">            -15.9094</td><td style=\"text-align: right;\">           184.721</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.2/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/5.81 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: C:\\Users\\xichao.chen\\ray_results\\PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_TradingEnv_9e021_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         121.218</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">  52.981</td><td style=\"text-align: right;\">             531.034</td><td style=\"text-align: right;\">            -15.9094</td><td style=\"text-align: right;\">           184.721</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "import tensortrade.env.default as default\n",
    "\n",
    "from tensortrade.feed.core import DataFeed, Stream\n",
    "from tensortrade.oms.exchanges import Exchange\n",
    "from tensortrade.oms.services.execution.simulated import execute_order\n",
    "from tensortrade.oms.wallets import Wallet, Portfolio\n",
    "\n",
    "\n",
    "\n",
    "def create_env(config):\n",
    "    x = np.arange(0, 2*np.pi, 2*np.pi / 1001)\n",
    "    y = 50*np.sin(3*x) + 100\n",
    "\n",
    "    x = np.arange(0, 2*np.pi, 2*np.pi / 1000)\n",
    "    p = Stream.source(y, dtype=\"float\").rename(\"USD-TTC\")\n",
    "\n",
    "    bitfinex = Exchange(\"bitfinex\", service=execute_order)(\n",
    "        p\n",
    "    )\n",
    "\n",
    "    cash = Wallet(bitfinex, 100000 * USD)\n",
    "    asset = Wallet(bitfinex, 0 * TTC)\n",
    "\n",
    "    portfolio = Portfolio(USD, [\n",
    "        cash,\n",
    "        asset\n",
    "    ])\n",
    "\n",
    "    feed = DataFeed([\n",
    "        p,\n",
    "        p.rolling(window=10).mean().rename(\"fast\"),\n",
    "        p.rolling(window=50).mean().rename(\"medium\"),\n",
    "        p.rolling(window=100).mean().rename(\"slow\"),\n",
    "        p.log().diff().fillna(0).rename(\"lr\")\n",
    "    ])\n",
    "\n",
    "    reward_scheme = PBR(price=p)\n",
    "\n",
    "    action_scheme = BSH(\n",
    "        cash=cash,\n",
    "        asset=asset\n",
    "    ).attach(reward_scheme)\n",
    "\n",
    "    renderer_feed = DataFeed([\n",
    "        Stream.source(y, dtype=\"float\").rename(\"price\"),\n",
    "        Stream.sensor(action_scheme, lambda s: s.action, dtype=\"float\").rename(\"action\")\n",
    "    ])\n",
    "\n",
    "    environment = default.create(\n",
    "        feed=feed,\n",
    "        portfolio=portfolio,\n",
    "        action_scheme=action_scheme,\n",
    "        reward_scheme=reward_scheme,\n",
    "        renderer_feed=renderer_feed,\n",
    "        renderer=PositionChangeChart(),\n",
    "        window_size=config[\"window_size\"],\n",
    "        max_allowed_loss=0.6\n",
    "    )\n",
    "    return environment\n",
    "\n",
    "register_env(\"TradingEnv\", create_env)\n",
    "\n",
    "\n",
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\n",
    "        \"episode_reward_mean\": 50\n",
    "    },\n",
    "    config={\n",
    "        \"env\": \"TradingEnv\",\n",
    "        \"env_config\": {\n",
    "            \"window_size\": 25\n",
    "        },\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"framework\": \"torch\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    },\n",
    "    checkpoint_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=7680)\u001B[0m WARNING:tensorflow:From c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\n",
      "\u001B[2m\u001B[36m(pid=7680)\u001B[0m Instructions for updating:\r\n",
      "\u001B[2m\u001B[36m(pid=7680)\u001B[0m non-resource variables are not supported in the long term\r\n",
      "\u001B[2m\u001B[36m(pid=7680)\u001B[0m 2021-04-18 15:32:12,815\tDEBUG rollout_worker.py:1075 -- Creating policy for default_policy\r\n",
      "\u001B[2m\u001B[36m(pid=7680)\u001B[0m 2021-04-18 15:32:12,816\tDEBUG catalog.py:618 -- Created preprocessor <ray.rllib.models.preprocessors.NoPreprocessor object at 0x0000021121CBB250>: Box(-inf, inf, (25, 5), float32) -> (25, 5)\r\n",
      "\u001B[2m\u001B[36m(pid=7680)\u001B[0m 2021-04-18 15:32:12,821\tWARNING deprecation.py:33 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\r\n",
      "\u001B[2m\u001B[36m(pid=7680)\u001B[0m 2021-04-18 15:32:12,862\tINFO torch_policy.py:109 -- TorchPolicy running on GPU.\r\n",
      "2021-04-18 15:32:15,446\tDEBUG rollout_worker.py:1075 -- Creating policy for default_policy\n",
      "2021-04-18 15:32:15,448\tDEBUG catalog.py:618 -- Created preprocessor <ray.rllib.models.preprocessors.NoPreprocessor object at 0x00000178B975CD60>: Box(-inf, inf, (25, 5), float32) -> (25, 5)\n",
      "2021-04-18 15:32:15,449\tWARNING deprecation.py:33 -- DeprecationWarning: `framestack` has been deprecated. Use `num_framestacks (int)` instead. This will raise an error in the future!\n",
      "\u001B[2m\u001B[36m(pid=7680)\u001B[0m 2021-04-18 15:32:15,440\tDEBUG rollout_worker.py:494 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\r\n",
      "\u001B[2m\u001B[36m(pid=7680)\u001B[0m 2021-04-18 15:32:15,440\tDEBUG rollout_worker.py:634 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x0000021121D06E20> (<TradingEnv instance>), policies {'default_policy': <ray.rllib.policy.policy_template.PPOTorchPolicy object at 0x0000021121CBBBE0>}\r\n",
      "2021-04-18 15:32:15,486\tINFO torch_policy.py:109 -- TorchPolicy running on GPU.\n",
      "2021-04-18 15:32:18,084\tINFO rollout_worker.py:1114 -- Built policy map: {'default_policy': <ray.rllib.policy.policy_template.PPOTorchPolicy object at 0x00000177E0B6D670>}\n",
      "2021-04-18 15:32:18,085\tINFO rollout_worker.py:1115 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x00000178B975CD60>}\n",
      "2021-04-18 15:32:18,086\tDEBUG rollout_worker.py:494 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "2021-04-18 15:32:18,087\tINFO rollout_worker.py:526 -- Built filter map: {'default_policy': MeanStdFilter((25, 5), True, True, None, (n=0, mean_mean=0.0, mean_std=0.0), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
      "2021-04-18 15:32:18,088\tDEBUG rollout_worker.py:634 -- Created rollout worker with env None (None), policies {'default_policy': <ray.rllib.policy.policy_template.PPOTorchPolicy object at 0x00000177E0B6D670>}\n",
      "2021-04-18 15:32:18,094\tINFO trainable.py:100 -- Trainable.setup took 12.387 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2021-04-18 15:32:18,095\tWARNING util.py:47 -- Install gputil for GPU system monitoring.\n",
      "2021-04-18 15:32:18,125\tINFO trainable.py:371 -- Restored on 192.168.30.36 from checkpoint: C:\\Users\\xichao.chen\\ray_results\\PPO\\PPO_TradingEnv_9e021_00000_0_2021-04-18_15-28-06\\checkpoint_4\\checkpoint-4\n",
      "2021-04-18 15:32:18,125\tINFO trainable.py:379 -- Current state after restoring: {'_iteration': 4, '_timesteps_total': None, '_time_total': 121.21757507324219, '_episodes_total': 86}\n"
     ]
    }
   ],
   "source": [
    "import ray.rllib.agents.ppo as ppo\n",
    "\n",
    "# Get checkpoint\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\", mode='min'),\n",
    "    metric=\"episode_reward_mean\"\n",
    ")\n",
    "checkpoint_path = checkpoints[0][0]\n",
    "\n",
    "# Restore agent\n",
    "agent = ppo.PPOTrainer(\n",
    "    env=\"TradingEnv\",\n",
    "    config={\n",
    "        \"env_config\": {\n",
    "            \"window_size\": 25\n",
    "        },\n",
    "        \"framework\": \"torch\",\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    }\n",
    ")\n",
    "agent.restore(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "Discrete(2)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = agent.get_policy()\n",
    "model = policy.model\n",
    "model.action_space"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "Box(-inf, inf, (25, 5), float32)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.obs_space"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "<bound method Module.modules of FullyConnectedNetwork(\n  (_logits): SlimFC(\n    (_model): Sequential(\n      (0): Linear(in_features=256, out_features=2, bias=True)\n    )\n  )\n  (_hidden_layers): Sequential(\n    (0): SlimFC(\n      (_model): Sequential(\n        (0): Linear(in_features=125, out_features=256, bias=True)\n        (1): Tanh()\n      )\n    )\n    (1): SlimFC(\n      (_model): Sequential(\n        (0): Linear(in_features=256, out_features=256, bias=True)\n        (1): Tanh()\n      )\n    )\n  )\n  (_value_branch_separate): Sequential(\n    (0): SlimFC(\n      (_model): Sequential(\n        (0): Linear(in_features=125, out_features=256, bias=True)\n        (1): Tanh()\n      )\n    )\n    (1): SlimFC(\n      (_model): Sequential(\n        (0): Linear(in_features=256, out_features=256, bias=True)\n        (1): Tanh()\n      )\n    )\n  )\n  (_value_branch): SlimFC(\n    (_model): Sequential(\n      (0): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n)>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.modules"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Instantiate the environment\n",
    "env = create_env({\n",
    "    \"window_size\": 25\n",
    "})\n",
    "\n",
    "# Run until episode ends\n",
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agent.compute_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "env.render()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# TensorTrade - Renderers and Plotly Visualization Chart\n",
    "## Data Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ipywidgets is required to run Plotly in Jupyter Notebook.\n",
    "# Uncomment and run the following line to install it if required.\n",
    "\n",
    "#!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensortrade.oms.instruments import Instrument\n",
    "\n",
    "USD = Instrument(\"USD\", 2, \"U.S. Dollar\")\n",
    "TTC = Instrument(\"TTC\", 8, \"TensorTrade Coin\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete\n",
    "\n",
    "from tensortrade.env.default.actions import TensorTradeActionScheme\n",
    "\n",
    "from tensortrade.env.generic import ActionScheme, TradingEnv\n",
    "from tensortrade.core import Clock\n",
    "from tensortrade.oms.instruments import ExchangePair\n",
    "from tensortrade.oms.wallets import Portfolio\n",
    "from tensortrade.oms.orders import (\n",
    "    Order,\n",
    "    proportion_order,\n",
    "    TradeSide,\n",
    "    TradeType\n",
    ")\n",
    "\n",
    "\n",
    "class BSH(TensorTradeActionScheme):\n",
    "\n",
    "    registered_name = \"bsh\"\n",
    "\n",
    "    def __init__(self, cash: 'Wallet', asset: 'Wallet'):\n",
    "        super().__init__()\n",
    "        self.cash = cash\n",
    "        self.asset = asset\n",
    "\n",
    "        self.listeners = []\n",
    "        self.action = 0\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return Discrete(2)\n",
    "\n",
    "    def attach(self, listener):\n",
    "        self.listeners += [listener]\n",
    "        return self\n",
    "\n",
    "    def get_orders(self, action: int, portfolio: 'Portfolio'):\n",
    "        order = None\n",
    "\n",
    "        if abs(action - self.action) > 0:\n",
    "            src = self.cash if self.action == 0 else self.asset\n",
    "            tgt = self.asset if self.action == 0 else self.cash\n",
    "            order = proportion_order(portfolio, src, tgt, 1.0)\n",
    "            self.action = action\n",
    "\n",
    "        for listener in self.listeners:\n",
    "            listener.on_action(action)\n",
    "\n",
    "        return [order]\n",
    "\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.action = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensortrade.env.default.rewards import TensorTradeRewardScheme\n",
    "from tensortrade.feed.core import Stream, DataFeed\n",
    "\n",
    "\n",
    "class PBR(TensorTradeRewardScheme):\n",
    "\n",
    "    registered_name = \"pbr\"\n",
    "\n",
    "    def __init__(self, price: 'Stream'):\n",
    "        super().__init__()\n",
    "        self.position = -1\n",
    "\n",
    "        r = Stream.sensor(price, lambda p: p.value, dtype=\"float\").diff()\n",
    "        position = Stream.sensor(self, lambda rs: rs.position, dtype=\"float\")\n",
    "\n",
    "        reward = (r * position).fillna(0).rename(\"reward\")\n",
    "\n",
    "        self.feed = DataFeed([reward])\n",
    "        self.feed.compile()\n",
    "\n",
    "    def on_action(self, action: int):\n",
    "        self.position = -1 if action == 0 else 1\n",
    "\n",
    "    def get_reward(self, portfolio: 'Portfolio'):\n",
    "        return self.feed.next()[\"reward\"]\n",
    "\n",
    "    def reset(self):\n",
    "        self.position = -1\n",
    "        self.feed.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensortrade.env.generic import Renderer\n",
    "\n",
    "\n",
    "class PositionChangeChart(Renderer):\n",
    "\n",
    "    def __init__(self, color: str = \"orange\"):\n",
    "        self.color = \"orange\"\n",
    "\n",
    "    def render(self, env, **kwargs):\n",
    "        history = pd.DataFrame(env.observer.renderer_history)\n",
    "\n",
    "        actions = list(history.action)\n",
    "        p = list(history.price)\n",
    "\n",
    "        buy = {}\n",
    "        sell = {}\n",
    "\n",
    "        for i in range(len(actions) - 1):\n",
    "            a1 = actions[i]\n",
    "            a2 = actions[i + 1]\n",
    "\n",
    "            if a1 != a2:\n",
    "                if a1 == 0 and a2 == 1:\n",
    "                    buy[i] = p[i]\n",
    "                else:\n",
    "                    sell[i] = p[i]\n",
    "\n",
    "        buy = pd.Series(buy)\n",
    "        sell = pd.Series(sell)\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        fig.suptitle(\"Performance\")\n",
    "\n",
    "        axs[0].plot(np.arange(len(p)), p, label=\"price\", color=self.color)\n",
    "        axs[0].scatter(buy.index, buy.values, marker=\"^\", color=\"green\")\n",
    "        axs[0].scatter(sell.index, sell.values, marker=\"^\", color=\"red\")\n",
    "        axs[0].set_title(\"Trading Chart\")\n",
    "\n",
    "        performance = pd.DataFrame.from_dict(env.action_scheme.portfolio.performance, orient='index')\n",
    "        performance.plot(ax=axs[1])\n",
    "        axs[1].set_title(\"Net Worth\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "import tensortrade.env.default as default\n",
    "\n",
    "from tensortrade.feed.core import DataFeed, Stream\n",
    "from tensortrade.oms.exchanges import Exchange\n",
    "from tensortrade.oms.services.execution.simulated import execute_order\n",
    "from tensortrade.oms.wallets import Wallet, Portfolio\n",
    "\n",
    "\n",
    "import ta\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tensortrade.feed.core import Stream, DataFeed, NameSpace\n",
    "from tensortrade.oms.exchanges import Exchange\n",
    "from tensortrade.oms.services.execution.simulated import execute_order\n",
    "from tensortrade.oms.instruments import USD, BTC\n",
    "from tensortrade.oms.wallets import Wallet, Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_csv(filename):\n",
    "    df = pd.read_csv('../data/' + filename, skiprows=1)\n",
    "    df.drop(columns=['symbol', 'volume_btc'], inplace=True)\n",
    "\n",
    "    # Fix timestamp form \"2019-10-17 09-AM\" to \"2019-10-17 09-00-00 AM\"\n",
    "    df['date'] = df['date'].str[:14] + '00-00 ' + df['date'].str[-2:]\n",
    "\n",
    "    # Convert the date column type from string to datetime for proper sorting.\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Make sure historical prices are sorted chronologically, oldest first.\n",
    "    df.sort_values(by='date', ascending=True, inplace=True)\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Format timestamps as you want them to appear on the chart buy/sell marks.\n",
    "    df['date'] = df['date'].dt.strftime('%Y-%m-%d %I:%M %p')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "C:\\Users\\xichao.chen\\work\\github\\tensortrade\\examples\\ray\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ta\\trend.py:768: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (self._dip[i] / self._trs[i])\n",
      "c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ta\\trend.py:772: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (self._din[i] / self._trs[i])\n"
     ]
    },
    {
     "data": {
      "text/plain": "      volume_adi  volume_obv  volume_cmf     volume_fi  volume_mfi  \\\n0  154659.537174   287000.32    0.538883  0.000000e+00        50.0   \n1 -141466.449196  -106142.18   -0.207995 -8.153775e+06         0.0   \n2 -833498.148276  -799396.19   -0.606888 -1.035618e+07         0.0   \n\n       volume_em  volume_sma_em     volume_vpt  volume_nvi  volume_vwap  ...  \\\n0       0.000000       0.000000 -148993.882445      1000.0  2505.890000  ...   \n1  -37727.185435  -37727.185435 -157499.832423      1000.0  2499.843813  ...   \n2 -135063.834683  -86395.510059  -12721.682056      1000.0  2482.635850  ...   \n\n   momentum_wr  momentum_ao  momentum_kama  momentum_roc  momentum_ppo  \\\n0   -23.055860          0.0    2509.170000           0.0      0.000000   \n1   -87.883057          0.0    2499.321489           0.0      2.871568   \n2   -99.949135          0.0    2478.576104           0.0     12.006246   \n\n   momentum_ppo_signal  momentum_ppo_hist  others_dr  others_dlr  others_cr  \n0             0.000000           0.000000 -53.745669    0.000000   0.000000  \n1             0.574314           2.297254  -0.826568   -0.830003  -0.826568  \n2             2.860700           9.145546  -1.366323   -1.375743  -2.181598  \n\n[3 rows x 83 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>volume_adi</th>\n      <th>volume_obv</th>\n      <th>volume_cmf</th>\n      <th>volume_fi</th>\n      <th>volume_mfi</th>\n      <th>volume_em</th>\n      <th>volume_sma_em</th>\n      <th>volume_vpt</th>\n      <th>volume_nvi</th>\n      <th>volume_vwap</th>\n      <th>...</th>\n      <th>momentum_wr</th>\n      <th>momentum_ao</th>\n      <th>momentum_kama</th>\n      <th>momentum_roc</th>\n      <th>momentum_ppo</th>\n      <th>momentum_ppo_signal</th>\n      <th>momentum_ppo_hist</th>\n      <th>others_dr</th>\n      <th>others_dlr</th>\n      <th>others_cr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>154659.537174</td>\n      <td>287000.32</td>\n      <td>0.538883</td>\n      <td>0.000000e+00</td>\n      <td>50.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-148993.882445</td>\n      <td>1000.0</td>\n      <td>2505.890000</td>\n      <td>...</td>\n      <td>-23.055860</td>\n      <td>0.0</td>\n      <td>2509.170000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-53.745669</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-141466.449196</td>\n      <td>-106142.18</td>\n      <td>-0.207995</td>\n      <td>-8.153775e+06</td>\n      <td>0.0</td>\n      <td>-37727.185435</td>\n      <td>-37727.185435</td>\n      <td>-157499.832423</td>\n      <td>1000.0</td>\n      <td>2499.843813</td>\n      <td>...</td>\n      <td>-87.883057</td>\n      <td>0.0</td>\n      <td>2499.321489</td>\n      <td>0.0</td>\n      <td>2.871568</td>\n      <td>0.574314</td>\n      <td>2.297254</td>\n      <td>-0.826568</td>\n      <td>-0.830003</td>\n      <td>-0.826568</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-833498.148276</td>\n      <td>-799396.19</td>\n      <td>-0.606888</td>\n      <td>-1.035618e+07</td>\n      <td>0.0</td>\n      <td>-135063.834683</td>\n      <td>-86395.510059</td>\n      <td>-12721.682056</td>\n      <td>1000.0</td>\n      <td>2482.635850</td>\n      <td>...</td>\n      <td>-99.949135</td>\n      <td>0.0</td>\n      <td>2478.576104</td>\n      <td>0.0</td>\n      <td>12.006246</td>\n      <td>2.860700</td>\n      <td>9.145546</td>\n      <td>-1.366323</td>\n      <td>-1.375743</td>\n      <td>-2.181598</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 83 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "print(\"------\")\n",
    "print(os.getcwd())\n",
    "\n",
    "df = load_csv('Coinbase_BTCUSD_1h.csv')\n",
    "df.head()\n",
    "\n",
    "dataset = ta.add_all_ta_features(df, 'open', 'high', 'low', 'close', 'volume', fillna=True)\n",
    "\n",
    "price_history = dataset[['date', 'open', 'high', 'low', 'close', 'volume']]  # chart data\n",
    "\n",
    "dataset.drop(columns=['date', 'open', 'high', 'low', 'close', 'volume'], inplace=True)\n",
    "\n",
    "dataset.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with NameSpace(\"bitfinex\"):\n",
    "    streams = [Stream.source(dataset[c].tolist(), dtype=\"float\").rename(c) for c in dataset.columns]\n",
    "\n",
    "feed_out = DataFeed(streams)\n",
    "#feed_out.next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-18 15:22:35,343\tINFO services.py:1172 -- View the Ray dashboard at \u001B[1m\u001B[32mhttp://127.0.0.1:8265\u001B[39m\u001B[22m\n",
      "2021-04-18 15:22:39,166\tWARNING worker.py:1107 -- The dashboard on node LAPTOP-20JJPQJR failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ray\\new_dashboard\\dashboard.py\", line 187, in <module>\n",
      "    dashboard = Dashboard(\n",
      "  File \"c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ray\\new_dashboard\\dashboard.py\", line 81, in __init__\n",
      "    build_dir = setup_static_dir()\n",
      "  File \"c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ray\\new_dashboard\\dashboard.py\", line 38, in setup_static_dir\n",
      "    raise OSError(\n",
      "FileNotFoundError: [Errno 2] Dashboard build directory not found. If installing from source, please follow the additional steps required to build the dashboard(cd python/ray/new_dashboard/client && npm install && npm ci && npm run build): 'c:\\\\users\\\\xichao.chen\\\\miniconda3\\\\envs\\\\python38\\\\lib\\\\site-packages\\\\ray\\\\new_dashboard\\\\client\\\\build'\n",
      "\n",
      "2021-04-18 15:22:42,228\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n",
      "\u001B[2m\u001B[36m(pid=9400)\u001B[0m WARNING:tensorflow:From c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\n",
      "\u001B[2m\u001B[36m(pid=9400)\u001B[0m Instructions for updating:\r\n",
      "\u001B[2m\u001B[36m(pid=9400)\u001B[0m non-resource variables are not supported in the long term\r\n",
      "\u001B[2m\u001B[36m(pid=14800)\u001B[0m WARNING:tensorflow:From c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\n",
      "\u001B[2m\u001B[36m(pid=14800)\u001B[0m Instructions for updating:\r\n",
      "\u001B[2m\u001B[36m(pid=14800)\u001B[0m non-resource variables are not supported in the long term\r\n",
      "2021-04-18 15:22:55,939\tERROR trial_runner.py:616 -- Trial PPO_TradingEnv_dcdbb_00000: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 586, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ray\\tune\\ray_trial_executor.py\", line 609, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 47, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ray\\worker.py\", line 1456, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(NotImplementedError): \u001B[36mray::PPO.train_buffered()\u001B[39m (pid=9400, ip=192.168.30.36)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 439, in ray._raylet.execute_task\n",
      "  File \"c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ray\\worker.py\", line 176, in reraise_actor_init_error\n",
      "    raise self.actor_init_error\n",
      "  File \"python\\ray\\_raylet.pyx\", line 473, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 476, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 480, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 432, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ray\\function_manager.py\", line 556, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ray\\rllib\\agents\\trainer_template.py\", line 107, in __init__\n",
      "    Trainer.__init__(self, config, env, logger_creator)\n",
      "  File \"c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py\", line 486, in __init__\n",
      "    super().__init__(config, logger_creator)\n",
      "  File \"c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ray\\tune\\trainable.py\", line 97, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py\", line 654, in setup\n",
      "    self._init(self.config, self.env_creator)\n",
      "  File \"c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ray\\rllib\\agents\\trainer_template.py\", line 134, in _init\n",
      "    self.workers = self._make_workers(\n",
      "  File \"c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py\", line 725, in _make_workers\n",
      "    return WorkerSet(\n",
      "  File \"c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\", line 79, in __init__\n",
      "    remote_spaces = ray.get(self.remote_workers(\n",
      "  File \"c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 47, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ray\\worker.py\", line 1456, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(NotImplementedError): \u001B[36mray::RolloutWorker.foreach_policy()\u001B[39m (pid=14800, ip=192.168.30.36)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 439, in ray._raylet.execute_task\n",
      "  File \"c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ray\\worker.py\", line 176, in reraise_actor_init_error\n",
      "    raise self.actor_init_error\n",
      "  File \"python\\ray\\_raylet.pyx\", line 473, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 476, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 480, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 432, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ray\\function_manager.py\", line 556, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"c:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 378, in __init__\n",
      "    self.env = _validate_env(env_creator(env_context))\n",
      "  File \"<ipython-input-10-1145695aa426>\", line 33, in create_env\n",
      "  File \"C:\\Users\\xichao.chen\\work\\github\\tensortrade\\tensortrade\\env\\default\\__init__.py\", line 56, in create\n",
      "    observer = observers.TensorTradeObserver(\n",
      "  File \"C:\\Users\\xichao.chen\\work\\github\\tensortrade\\tensortrade\\core\\component.py\", line 41, in __call__\n",
      "    instance.__init__(*args, **kwargs)\n",
      "  File \"C:\\Users\\xichao.chen\\work\\github\\tensortrade\\tensortrade\\env\\default\\observers.py\", line 218, in __init__\n",
      "    initial_obs = self.feed.next()[\"external\"]\n",
      "  File \"C:\\Users\\xichao.chen\\work\\github\\tensortrade\\tensortrade\\feed\\core\\feed.py\", line 51, in next\n",
      "    self.run()\n",
      "  File \"C:\\Users\\xichao.chen\\work\\github\\tensortrade\\tensortrade\\feed\\core\\feed.py\", line 43, in run\n",
      "    s.run()\n",
      "  File \"C:\\Users\\xichao.chen\\work\\github\\tensortrade\\tensortrade\\feed\\core\\base.py\", line 169, in run\n",
      "    self.value = self.forward()\n",
      "  File \"C:\\Users\\xichao.chen\\work\\github\\tensortrade\\tensortrade\\feed\\core\\base.py\", line 182, in forward\n",
      "    raise NotImplementedError()\n",
      "NotImplementedError\n",
      "2021-04-18 15:22:55,955\tWARNING trial_runner.py:420 -- Trial Runner checkpointing failed: [WinError 183] 当文件已存在时，无法创建该文件。: 'C:\\\\Users\\\\xichao.chen\\\\ray_results\\\\PPO\\\\.tmp_generator' -> 'C:\\\\Users\\\\xichao.chen\\\\ray_results\\\\PPO\\\\basic-variant-state-2021-04-18_15-22-42.json'\n",
      "2021-04-18 15:22:55,965\tWARNING tune.py:429 -- Trial Runner checkpointing failed: [WinError 183] 当文件已存在时，无法创建该文件。: 'C:\\\\Users\\\\xichao.chen\\\\ray_results\\\\PPO\\\\.tmp_generator' -> 'C:\\\\Users\\\\xichao.chen\\\\ray_results\\\\PPO\\\\basic-variant-state-2021-04-18_15-22-42.json'\n",
      "\u001B[2m\u001B[36m(pid=14800)\u001B[0m Windows fatal exception: access violation\n",
      "\u001B[2m\u001B[36m(pid=14800)\u001B[0m \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 6.6/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 0/1 GPUs, 0.0/5.91 GiB heap, 0.0/2.05 GiB objects<br>Result logdir: C:\\Users\\xichao.chen\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                </th><th>status  </th><th>loc  </th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_TradingEnv_dcdbb_00000</td><td>RUNNING </td><td>     </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TradingEnv_dcdbb_00000:\n",
      "  {}\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 7.1/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/5.91 GiB heap, 0.0/2.05 GiB objects<br>Result logdir: C:\\Users\\xichao.chen\\ray_results\\PPO<br>Number of trials: 1/1 (1 ERROR)<br><table>\n<thead>\n<tr><th>Trial name                </th><th>status  </th><th>loc  </th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_TradingEnv_dcdbb_00000</td><td>ERROR   </td><td>     </td></tr>\n</tbody>\n</table><br>Number of errored trials: 1<br><table>\n<thead>\n<tr><th>Trial name                </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                     </th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_TradingEnv_dcdbb_00000</td><td style=\"text-align: right;\">           1</td><td>C:\\Users\\xichao.chen\\ray_results\\PPO\\PPO_TradingEnv_dcdbb_00000_0_2021-04-18_15-22-42\\error.txt</td></tr>\n</tbody>\n</table><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 7.1/15.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/5.91 GiB heap, 0.0/2.05 GiB objects<br>Result logdir: C:\\Users\\xichao.chen\\ray_results\\PPO<br>Number of trials: 1/1 (1 ERROR)<br><table>\n<thead>\n<tr><th>Trial name                </th><th>status  </th><th>loc  </th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_TradingEnv_dcdbb_00000</td><td>ERROR   </td><td>     </td></tr>\n</tbody>\n</table><br>Number of errored trials: 1<br><table>\n<thead>\n<tr><th>Trial name                </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                     </th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_TradingEnv_dcdbb_00000</td><td style=\"text-align: right;\">           1</td><td>C:\\Users\\xichao.chen\\ray_results\\PPO\\PPO_TradingEnv_dcdbb_00000_0_2021-04-18_15-22-42\\error.txt</td></tr>\n</tbody>\n</table><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [PPO_TradingEnv_dcdbb_00000])",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTuneError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-10-1145695aa426>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 48\u001B[1;33m analysis = tune.run(\n\u001B[0m\u001B[0;32m     49\u001B[0m     \u001B[1;34m\"PPO\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     50\u001B[0m     stop={\n",
      "\u001B[1;32mc:\\users\\xichao.chen\\miniconda3\\envs\\python38\\lib\\site-packages\\ray\\tune\\tune.py\u001B[0m in \u001B[0;36mrun\u001B[1;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint)\u001B[0m\n\u001B[0;32m    442\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mincomplete_trials\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    443\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mraise_on_failed_trial\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 444\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mTuneError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Trials did not complete\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mincomplete_trials\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    445\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    446\u001B[0m             \u001B[0mlogger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0merror\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Trials did not complete: %s\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mincomplete_trials\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTuneError\u001B[0m: ('Trials did not complete', [PPO_TradingEnv_dcdbb_00000])"
     ]
    }
   ],
   "source": [
    "def create_env(config):\n",
    "    price_list = price_history['close'].tolist()\n",
    "    price_stream = Stream.source(price_list, dtype=\"float\").rename(\"USD-BTC\")\n",
    "    bitfinex = Exchange(\"bitfinex\", service=execute_order)(\n",
    "        price_stream\n",
    "    )\n",
    "    \n",
    "    cash = Wallet(bitfinex, 100000 * USD)\n",
    "    asset = Wallet(bitfinex, 0 * BTC)\n",
    "    \n",
    "    portfolio = Portfolio(USD, [\n",
    "        cash,\n",
    "        asset,\n",
    "    ])\n",
    "\n",
    "    with NameSpace(\"bitfinex\"):\n",
    "        streams = [Stream.source(dataset[c].tolist(), dtype=\"float\").rename(c) for c in dataset.columns]\n",
    "\n",
    "    feed = feed_out\n",
    "\n",
    "    reward_scheme = PBR(price=price_stream)\n",
    "\n",
    "    action_scheme = BSH(\n",
    "        cash=cash,\n",
    "        asset=asset\n",
    "    ).attach(reward_scheme)\n",
    "\n",
    "    renderer_feed = DataFeed([\n",
    "        Stream.source(price_list, dtype=\"float\").rename(\"price\"),\n",
    "        Stream.sensor(action_scheme, lambda s: s.action, dtype=\"float\").rename(\"action\")\n",
    "    ])\n",
    "\n",
    "    environment = default.create(\n",
    "        feed=feed,\n",
    "        portfolio=portfolio,\n",
    "        action_scheme=action_scheme,\n",
    "        reward_scheme=reward_scheme,\n",
    "        renderer_feed=renderer_feed,\n",
    "        renderer=PositionChangeChart(),\n",
    "        window_size=config[\"window_size\"],\n",
    "        max_allowed_loss=0.6\n",
    "    )\n",
    "    return environment\n",
    "\n",
    "register_env(\"TradingEnv\", create_env)\n",
    "\n",
    "\n",
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\n",
    "        \"episode_reward_mean\": 50\n",
    "    },\n",
    "    config={\n",
    "        \"env\": \"TradingEnv\",\n",
    "        \"env_config\": {\n",
    "            \"window_size\": 25\n",
    "        },\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"framework\": \"torch\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    },\n",
    "    checkpoint_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ray.rllib.agents.ppo as ppo\n",
    "\n",
    "# Get checkpoint\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\", mode='min'),\n",
    "    metric=\"episode_reward_mean\"\n",
    ")\n",
    "checkpoint_path = checkpoints[0][0]\n",
    "\n",
    "# Restore agent\n",
    "agent = ppo.PPOTrainer(\n",
    "    env=\"TradingEnv\",\n",
    "    config={\n",
    "        \"env_config\": {\n",
    "            \"window_size\": 25\n",
    "        },\n",
    "        \"framework\": \"torch\",\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    }\n",
    ")\n",
    "agent.restore(checkpoint_path)\n",
    "\n",
    "# Instantiate the environment\n",
    "env = create_env({\n",
    "    \"window_size\": 25\n",
    "})\n",
    "\n",
    "# Run until episode ends\n",
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agent.compute_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ray.rllib.agents.ppo as ppo\n",
    "\n",
    "# Get checkpoint\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\", mode='min'),\n",
    "    metric=\"episode_reward_mean\"\n",
    ")\n",
    "checkpoint_path = checkpoints[0][0]\n",
    "\n",
    "# Restore agent\n",
    "agent = ppo.PPOTrainer(\n",
    "    env=\"TradingEnv\",\n",
    "    config={\n",
    "        \"env_config\": {\n",
    "            \"window_size\": 25\n",
    "        },\n",
    "        \"framework\": \"torch\",\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    }\n",
    ")\n",
    "agent.restore(checkpoint_path)\n",
    "\n",
    "# Instantiate the environment\n",
    "env = create_env({\n",
    "    \"window_size\": 25\n",
    "})\n",
    "\n",
    "# Run until episode ends\n",
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agent.compute_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ray.rllib.agents.ppo as ppo\n",
    "\n",
    "# Get checkpoint\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\", mode='min'),\n",
    "    metric=\"episode_reward_mean\"\n",
    ")\n",
    "checkpoint_path = checkpoints[0][0]\n",
    "\n",
    "# Restore agent\n",
    "agent = ppo.PPOTrainer(\n",
    "    env=\"TradingEnv\",\n",
    "    config={\n",
    "        \"env_config\": {\n",
    "            \"window_size\": 25\n",
    "        },\n",
    "        \"framework\": \"torch\",\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    }\n",
    ")\n",
    "agent.restore(checkpoint_path)\n",
    "\n",
    "# Instantiate the environment\n",
    "env = create_env({\n",
    "    \"window_size\": 25\n",
    "})\n",
    "\n",
    "# Run until episode ends\n",
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agent.compute_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ray.rllib.agents.ppo as ppo\n",
    "\n",
    "# Get checkpoint\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\", mode='min'),\n",
    "    metric=\"episode_reward_mean\"\n",
    ")\n",
    "checkpoint_path = checkpoints[0][0]\n",
    "\n",
    "# Restore agent\n",
    "agent = ppo.PPOTrainer(\n",
    "    env=\"TradingEnv\",\n",
    "    config={\n",
    "        \"env_config\": {\n",
    "            \"window_size\": 25\n",
    "        },\n",
    "        \"framework\": \"torch\",\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    }\n",
    ")\n",
    "agent.restore(checkpoint_path)\n",
    "\n",
    "# Instantiate the environment\n",
    "env = create_env({\n",
    "    \"window_size\": 25\n",
    "})\n",
    "\n",
    "# Run until episode ends\n",
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agent.compute_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ray.rllib.agents.ppo as ppo\n",
    "\n",
    "# Get checkpoint\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\", mode='min'),\n",
    "    metric=\"episode_reward_mean\"\n",
    ")\n",
    "checkpoint_path = checkpoints[0][0]\n",
    "\n",
    "# Restore agent\n",
    "agent = ppo.PPOTrainer(\n",
    "    env=\"TradingEnv\",\n",
    "    config={\n",
    "        \"env_config\": {\n",
    "            \"window_size\": 25\n",
    "        },\n",
    "        \"framework\": \"torch\",\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    }\n",
    ")\n",
    "agent.restore(checkpoint_path)\n",
    "\n",
    "# Instantiate the environment\n",
    "env = create_env({\n",
    "    \"window_size\": 25\n",
    "})\n",
    "\n",
    "# Run until episode ends\n",
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agent.compute_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"------\")\n",
    "print(os.getcwd())\n",
    "\n",
    "df = load_csv('Coinbase_BTCUSD_1h.csv')\n",
    "df.head()\n",
    "\n",
    "def create_env(config):\n",
    "    x = np.arange(0, 2*np.pi, 2*np.pi / 1001)\n",
    "    y = 50*np.sin(3*x) + 100\n",
    "\n",
    "    x = np.arange(0, 2*np.pi, 2*np.pi / 1000)\n",
    "    p = Stream.source(y, dtype=\"float\").rename(\"USD-TTC\")\n",
    "\n",
    "    bitfinex = Exchange(\"bitfinex\", service=execute_order)(\n",
    "        p\n",
    "    )\n",
    "\n",
    "    cash = Wallet(bitfinex, 100000 * USD)\n",
    "    asset = Wallet(bitfinex, 0 * TTC)\n",
    "\n",
    "    portfolio = Portfolio(USD, [\n",
    "        cash,\n",
    "        asset\n",
    "    ])\n",
    "\n",
    "    feed = DataFeed([\n",
    "        p,\n",
    "        p.rolling(window=10).mean().rename(\"fast\"),\n",
    "        p.rolling(window=50).mean().rename(\"medium\"),\n",
    "        p.rolling(window=100).mean().rename(\"slow\"),\n",
    "        p.log().diff().fillna(0).rename(\"lr\")\n",
    "    ])\n",
    "\n",
    "    reward_scheme = PBR(price=p)\n",
    "\n",
    "    action_scheme = BSH(\n",
    "        cash=cash,\n",
    "        asset=asset\n",
    "    ).attach(reward_scheme)\n",
    "\n",
    "    renderer_feed = DataFeed([\n",
    "        Stream.source(y, dtype=\"float\").rename(\"price\"),\n",
    "        Stream.sensor(action_scheme, lambda s: s.action, dtype=\"float\").rename(\"action\")\n",
    "    ])\n",
    "\n",
    "    environment = default.create(\n",
    "        feed=feed,\n",
    "        portfolio=portfolio,\n",
    "        action_scheme=action_scheme,\n",
    "        reward_scheme=reward_scheme,\n",
    "        renderer_feed=renderer_feed,\n",
    "        renderer=PositionChangeChart(),\n",
    "        window_size=config[\"window_size\"],\n",
    "        max_allowed_loss=0.6\n",
    "    )\n",
    "    return environment\n",
    "\n",
    "register_env(\"TradingEnv\", create_env)\n",
    "\n",
    "\n",
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\n",
    "        \"episode_reward_mean\": 50\n",
    "    },\n",
    "    config={\n",
    "        \"env\": \"TradingEnv\",\n",
    "        \"env_config\": {\n",
    "            \"window_size\": 25\n",
    "        },\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"framework\": \"torch\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    },\n",
    "    checkpoint_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ray.rllib.agents.ppo as ppo\n",
    "\n",
    "# Get checkpoint\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\", mode='min'),\n",
    "    metric=\"episode_reward_mean\"\n",
    ")\n",
    "checkpoint_path = checkpoints[0][0]\n",
    "\n",
    "# Restore agent\n",
    "agent = ppo.PPOTrainer(\n",
    "    env=\"TradingEnv\",\n",
    "    config={\n",
    "        \"env_config\": {\n",
    "            \"window_size\": 25\n",
    "        },\n",
    "        \"framework\": \"torch\",\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    }\n",
    ")\n",
    "agent.restore(checkpoint_path)\n",
    "\n",
    "# Instantiate the environment\n",
    "env = create_env({\n",
    "    \"window_size\": 25\n",
    "})\n",
    "\n",
    "# Run until episode ends\n",
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agent.compute_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"------\")\n",
    "print(os.getcwd())\n",
    "\n",
    "df = load_csv('Coinbase_BTCUSD_1h.csv')\n",
    "df.head()\n",
    "\n",
    "def create_env(config):\n",
    "    x = np.arange(0, 2*np.pi, 2*np.pi / 1001)\n",
    "    y = 50*np.sin(3*x) + 100\n",
    "\n",
    "    x = np.arange(0, 2*np.pi, 2*np.pi / 1000)\n",
    "    p = Stream.source(y, dtype=\"float\").rename(\"USD-TTC\")\n",
    "\n",
    "    bitfinex = Exchange(\"bitfinex\", service=execute_order)(\n",
    "        p\n",
    "    )\n",
    "\n",
    "    cash = Wallet(bitfinex, 100000 * USD)\n",
    "    asset = Wallet(bitfinex, 0 * TTC)\n",
    "\n",
    "    portfolio = Portfolio(USD, [\n",
    "        cash,\n",
    "        asset\n",
    "    ])\n",
    "\n",
    "    feed = DataFeed([\n",
    "        p,\n",
    "        p.rolling(window=10).mean().rename(\"fast\"),\n",
    "        p.rolling(window=50).mean().rename(\"medium\"),\n",
    "        p.rolling(window=100).mean().rename(\"slow\"),\n",
    "        p.log().diff().fillna(0).rename(\"lr\")\n",
    "    ])\n",
    "\n",
    "    reward_scheme = PBR(price=p)\n",
    "\n",
    "    action_scheme = BSH(\n",
    "        cash=cash,\n",
    "        asset=asset\n",
    "    ).attach(reward_scheme)\n",
    "\n",
    "    renderer_feed = DataFeed([\n",
    "        Stream.source(y, dtype=\"float\").rename(\"price\"),\n",
    "        Stream.sensor(action_scheme, lambda s: s.action, dtype=\"float\").rename(\"action\")\n",
    "    ])\n",
    "\n",
    "    environment = default.create(\n",
    "        feed=feed,\n",
    "        portfolio=portfolio,\n",
    "        action_scheme=action_scheme,\n",
    "        reward_scheme=reward_scheme,\n",
    "        renderer_feed=renderer_feed,\n",
    "        renderer=PositionChangeChart(),\n",
    "        window_size=config[\"window_size\"],\n",
    "        max_allowed_loss=0.6\n",
    "    )\n",
    "    return environment\n",
    "\n",
    "register_env(\"TradingEnv\", create_env)\n",
    "\n",
    "\n",
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\n",
    "        \"episode_reward_mean\": 50\n",
    "    },\n",
    "    config={\n",
    "        \"env\": \"TradingEnv\",\n",
    "        \"env_config\": {\n",
    "            \"window_size\": 25\n",
    "        },\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"framework\": \"torch\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    },\n",
    "    checkpoint_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ray.rllib.agents.ppo as ppo\n",
    "\n",
    "# Get checkpoint\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\", mode='min'),\n",
    "    metric=\"episode_reward_mean\"\n",
    ")\n",
    "checkpoint_path = checkpoints[0][0]\n",
    "\n",
    "# Restore agent\n",
    "agent = ppo.PPOTrainer(\n",
    "    env=\"TradingEnv\",\n",
    "    config={\n",
    "        \"env_config\": {\n",
    "            \"window_size\": 25\n",
    "        },\n",
    "        \"framework\": \"torch\",\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    }\n",
    ")\n",
    "agent.restore(checkpoint_path)\n",
    "\n",
    "# Instantiate the environment\n",
    "env = create_env({\n",
    "    \"window_size\": 25\n",
    "})\n",
    "\n",
    "# Run until episode ends\n",
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agent.compute_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"------\")\n",
    "print(os.getcwd())\n",
    "\n",
    "df = load_csv('Coinbase_BTCUSD_1h.csv')\n",
    "df.head()\n",
    "\n",
    "def create_env(config):\n",
    "    x = np.arange(0, 2*np.pi, 2*np.pi / 1001)\n",
    "    y = 50*np.sin(3*x) + 100\n",
    "\n",
    "    x = np.arange(0, 2*np.pi, 2*np.pi / 1000)\n",
    "    p = Stream.source(y, dtype=\"float\").rename(\"USD-TTC\")\n",
    "\n",
    "    bitfinex = Exchange(\"bitfinex\", service=execute_order)(\n",
    "        p\n",
    "    )\n",
    "\n",
    "    cash = Wallet(bitfinex, 100000 * USD)\n",
    "    asset = Wallet(bitfinex, 0 * TTC)\n",
    "\n",
    "    portfolio = Portfolio(USD, [\n",
    "        cash,\n",
    "        asset\n",
    "    ])\n",
    "\n",
    "    feed = DataFeed([\n",
    "        p,\n",
    "        p.rolling(window=10).mean().rename(\"fast\"),\n",
    "        p.rolling(window=50).mean().rename(\"medium\"),\n",
    "        p.rolling(window=100).mean().rename(\"slow\"),\n",
    "        p.log().diff().fillna(0).rename(\"lr\")\n",
    "    ])\n",
    "\n",
    "    reward_scheme = PBR(price=p)\n",
    "\n",
    "    action_scheme = BSH(\n",
    "        cash=cash,\n",
    "        asset=asset\n",
    "    ).attach(reward_scheme)\n",
    "\n",
    "    renderer_feed = DataFeed([\n",
    "        Stream.source(y, dtype=\"float\").rename(\"price\"),\n",
    "        Stream.sensor(action_scheme, lambda s: s.action, dtype=\"float\").rename(\"action\")\n",
    "    ])\n",
    "\n",
    "    environment = default.create(\n",
    "        feed=feed,\n",
    "        portfolio=portfolio,\n",
    "        action_scheme=action_scheme,\n",
    "        reward_scheme=reward_scheme,\n",
    "        renderer_feed=renderer_feed,\n",
    "        renderer=PositionChangeChart(),\n",
    "        window_size=config[\"window_size\"],\n",
    "        max_allowed_loss=0.6\n",
    "    )\n",
    "    return environment\n",
    "\n",
    "register_env(\"TradingEnv\", create_env)\n",
    "\n",
    "\n",
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\n",
    "        \"episode_reward_mean\": 50\n",
    "    },\n",
    "    config={\n",
    "        \"env\": \"TradingEnv\",\n",
    "        \"env_config\": {\n",
    "            \"window_size\": 25\n",
    "        },\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"framework\": \"torch\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    },\n",
    "    checkpoint_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ray.rllib.agents.ppo as ppo\n",
    "\n",
    "# Get checkpoint\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\", mode='min'),\n",
    "    metric=\"episode_reward_mean\"\n",
    ")\n",
    "checkpoint_path = checkpoints[0][0]\n",
    "\n",
    "# Restore agent\n",
    "agent = ppo.PPOTrainer(\n",
    "    env=\"TradingEnv\",\n",
    "    config={\n",
    "        \"env_config\": {\n",
    "            \"window_size\": 25\n",
    "        },\n",
    "        \"framework\": \"torch\",\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    }\n",
    ")\n",
    "agent.restore(checkpoint_path)\n",
    "\n",
    "# Instantiate the environment\n",
    "env = create_env({\n",
    "    \"window_size\": 25\n",
    "})\n",
    "\n",
    "# Run until episode ends\n",
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agent.compute_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from symfit import parameters, variables, sin, cos, Fit\n",
    "\n",
    "\n",
    "def fourier_series(x, f, n=0):\n",
    "    \"\"\"Creates a symbolic fourier series of order `n`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : `symfit.Variable`\n",
    "        The input variable for the function.\n",
    "    f : `symfit.Parameter`\n",
    "        Frequency of the fourier series\n",
    "    n : int\n",
    "        Order of the fourier series.\n",
    "    \"\"\"\n",
    "    # Make the parameter objects for all the terms\n",
    "    a0, *cos_a = parameters(','.join(['a{}'.format(i) for i in range(0, n + 1)]))\n",
    "    sin_b = parameters(','.join(['b{}'.format(i) for i in range(1, n + 1)]))\n",
    "\n",
    "    # Construct the series\n",
    "    series = a0 + sum(ai * cos(i * f * x) + bi * sin(i * f * x)\n",
    "                     for i, (ai, bi) in enumerate(zip(cos_a, sin_b), start=1))\n",
    "    return series\n",
    "\n",
    "\n",
    "def gbm(price: float,\n",
    "        mu: float,\n",
    "        sigma: float,\n",
    "        dt: float,\n",
    "        n: int) -> np.array:\n",
    "    \"\"\"Generates a geometric brownian motion path.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    price : float\n",
    "        The initial price of the series.\n",
    "    mu : float\n",
    "        The percentage drift.\n",
    "    sigma : float\n",
    "        The percentage volatility.\n",
    "    dt : float\n",
    "        The time step size.\n",
    "    n : int\n",
    "        The number of steps to be generated in the path.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `np.array`\n",
    "        The generated path.\n",
    "    \"\"\"\n",
    "    y = np.exp((mu - sigma ** 2 / 2) * dt + sigma * np.random.normal(0, np.sqrt(dt), size=n).T)\n",
    "    y = price * y.cumprod(axis=0)\n",
    "    return y\n",
    "\n",
    "\n",
    "def fourier_gbm(price, mu, sigma, dt, n, order):\n",
    "\n",
    "    x, y = variables('x, y')\n",
    "    w, = parameters('w')\n",
    "    model_dict = {y: fourier_series(x, f=w, n=order)}\n",
    "\n",
    "    # Make step function data\n",
    "    xdata = np.arange(-np.pi, np.pi, 2*np.pi / n)\n",
    "    ydata = np.log(gbm(price, mu, sigma, dt, n))\n",
    "\n",
    "    # Define a Fit object for this model and data\n",
    "    fit = Fit(model_dict, x=xdata, y=ydata)\n",
    "    fit_result = fit.execute()\n",
    "\n",
    "    return np.exp(fit.model(x=xdata, **fit_result.params).y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_eval_env(config):\n",
    "    y = config[\"y\"]\n",
    "\n",
    "    x = np.arange(0, 2*np.pi, 2*np.pi / 1000)\n",
    "    p = Stream.source(y, dtype=\"float\").rename(\"USD-TTC\")\n",
    "\n",
    "    bitfinex = Exchange(\"bitfinex\", service=execute_order)(\n",
    "        p\n",
    "    )\n",
    "\n",
    "    cash = Wallet(bitfinex, 100000 * USD)\n",
    "    asset = Wallet(bitfinex, 0 * TTC)\n",
    "\n",
    "    portfolio = Portfolio(USD, [\n",
    "        cash,\n",
    "        asset\n",
    "    ])\n",
    "\n",
    "    feed = DataFeed([\n",
    "        p,\n",
    "        p.rolling(window=10).mean().rename(\"fast\"),\n",
    "        p.rolling(window=50).mean().rename(\"medium\"),\n",
    "        p.rolling(window=100).mean().rename(\"slow\"),\n",
    "        p.log().diff().fillna(0).rename(\"lr\")\n",
    "    ])\n",
    "\n",
    "    reward_scheme = PBR(price=p)\n",
    "\n",
    "    action_scheme = BSH(\n",
    "        cash=cash,\n",
    "        asset=asset\n",
    "    ).attach(reward_scheme)\n",
    "\n",
    "    renderer_feed = DataFeed([\n",
    "        Stream.source(y, dtype=\"float\").rename(\"price\"),\n",
    "        Stream.sensor(action_scheme, lambda s: s.action, dtype=\"float\").rename(\"action\")\n",
    "    ])\n",
    "\n",
    "    environment = default.create(\n",
    "        feed=feed,\n",
    "        portfolio=portfolio,\n",
    "        action_scheme=action_scheme,\n",
    "        reward_scheme=reward_scheme,\n",
    "        renderer_feed=renderer_feed,\n",
    "        renderer=PositionChangeChart(),\n",
    "        window_size=config[\"window_size\"],\n",
    "        max_allowed_loss=0.6\n",
    "    )\n",
    "    return environment\n",
    "\n",
    "# Instantiate the environment\n",
    "env = create_eval_env({\n",
    "    \"window_size\": 25,\n",
    "    \"y\": fourier_gbm(price=100, mu=0.01, sigma=0.5, dt=0.01, n=1000, order=5)\n",
    "})\n",
    "\n",
    "\n",
    "# Run until episode ends\n",
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agent.compute_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "    \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}